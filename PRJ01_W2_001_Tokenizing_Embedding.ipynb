{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP ê¸°ì´ˆ: Tokenization + Embedding ì´í•´"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### í•™ìŠµ ëª©í‘œ\n",
    "\n",
    "1. **í† í°í™”(Tokenization) ì´í•´**\n",
    "   - ë‹¨ì–´ ë‹¨ìœ„ í† í°í™” (í˜•íƒœì†Œ ë¶„ì„)\n",
    "   - ì„œë¸Œì›Œë“œ í† í°í™” (WordPiece, SentencePiece)\n",
    "   - í•œêµ­ì–´ í† í°í™” ë„êµ¬ í™œìš© (Kiwi, BERT, BGE-M3)\n",
    "\n",
    "2. **ì„ë² ë”©(Embedding) ì´í•´**\n",
    "   - ë‹¨ì–´ ì„ë² ë”© (BoW, TF-IDF, Word2Vec)\n",
    "   - ë¬¸ì¥ ì„ë² ë”© (í‰ê·  ê¸°ë°˜, SBERT)\n",
    "   - ì„ë² ë”© ë²¡í„°ì˜ ì˜ë¯¸ì™€ í™œìš©\n",
    "\n",
    "3. **í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ì¸¡ì •**\n",
    "   - ìœ ì‚¬ë„ ë©”íŠ¸ë¦­ (ìœ í´ë¦¬ë“œ ê±°ë¦¬, ì½”ì‚¬ì¸ ìœ ì‚¬ë„, ë‚´ì )\n",
    "   - ì‹¤ì œ ë¬¸ì¥ ê°„ ìœ ì‚¬ë„ ë¹„êµ\n",
    "   - ìœ ì‚¬ë„ ê²°ê³¼ ì‹œê°í™”\n",
    "\n",
    "### í•„ìš” íŒ¨í‚¤ì§€\n",
    "\n",
    "```bash\n",
    "# uv í™˜ê²½\n",
    "uv add kiwipiepy torch transformers scikit-learn gensim sentence-transformers matplotlib seaborn\n",
    "\n",
    "- ìœ„ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜ ì‹œ ì—ëŸ¬\n",
    "```yaml\n",
    "Resolved 180 packages in 14.65s\n",
    "error: Distribution `torch==2.10.0 @ registry+https://pypi.org/simple` can't be installed because it doesn't have a source distribution or wheel for the current platform\n",
    "\n",
    "hint: You're on macOS (`macosx_13_0_x86_64`), but `torch` (v2.10.0) only has wheels for the following platforms: `manylinux_2_28_aarch64`, `manylinux_2_28_x86_64`, `macosx_11_0_arm64`, `win_amd64`; consider adding \"sys_platform == 'darwin' and platform_machine == 'x86_64'\" to `tool.uv.required-environments` to ensure uv resolves to a version with compatible wheels\n",
    "\n",
    "\n",
    "```bash\n",
    "uv add kiwipiepy torch==2.2.2 transformers scikit-learn gensim sentence-transformers matplotlib seaborn\n",
    "\n",
    "\n",
    "Resolved 173 packages in 686ms\n",
    "  Ã— Failed to download `torch==2.2.2`\n",
    "  â”œâ”€â–¶ Failed to extract archive: torch-2.2.2-cp312-none-macosx_10_9_x86_64.whl\n",
    "  â”œâ”€â–¶ I/O operation failed during extraction\n",
    "  â•°â”€â–¶ failed to write to file `/Users/greenpianorabbit/.cache/uv/.tmp9Ii6r0/torch/lib/libtorch_cpu.dylib`: No space left on device (os error 28)\n",
    "  help: If you want to add the package regardless of the failed resolution, provide the `--frozen` flag to skip locking and syncing.\n",
    "\n",
    "\n",
    "# pip í™˜ê²½\n",
    "pip install kiwipiepy torch transformers scikit-learn gensim sentence-transformers matplotlib seaborn\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenization (í† í°í™”)\n",
    "\n",
    "- **ê°œë…:** í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„ ê°€ëŠ¥í•œ ì‘ì€ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ëŠ” ê³¼ì •ì´ë‹¤. ì»´í“¨í„°ê°€ ìì—°ì–´ë¥¼ ì´í•´í•˜ê¸° ìœ„í•œ ì²« ë²ˆì§¸ ë‹¨ê³„ë¡œ, ì˜ë¯¸ ìˆëŠ” ìµœì†Œ ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•œë‹¤.\n",
    "\n",
    "    - ë‹¨ì–´ ë‹¨ìœ„ í† í°í™” (Word Tokenization)\n",
    "    - ì„œë¸Œì›Œë“œ í† í°í™” (Subword Tokenization)\n",
    "\n",
    "### 1.1 **ë‹¨ì–´ ë‹¨ìœ„ í† í°í™” (Word Tokenization)**\n",
    "\n",
    "- **íŠ¹ì§•:**\n",
    "    - í˜•íƒœì†Œ ë¶„ì„ì„ ê¸°ë°˜ìœ¼ë¡œ ì˜ë¯¸ ìˆëŠ” ìµœì†Œ ë‹¨ìœ„ë¡œ ë¶„ë¦¬\n",
    "    - ì¡°ì‚¬, ì–´ë¯¸ ë“± ë¬¸ë²•ì  ìš”ì†Œë„ ê°œë³„ í† í°ìœ¼ë¡œ ë¶„ë¦¬\n",
    "    - í•œêµ­ì–´ì˜ êµì°©ì–´ íŠ¹ì„±ì„ ì˜ ë°˜ì˜\n",
    "\n",
    "- **í™œìš© ë¶„ì•¼:**\n",
    "    - ë¬¸ì¥ ë¶„ì„, í’ˆì‚¬ íƒœê¹…\n",
    "    - í…ìŠ¤íŠ¸ ë¶„ë¥˜, ê°ì„± ë¶„ì„\n",
    "    - êµ¬ë¬¸ ë¶„ì„, ì˜ì¡´ êµ¬ë¬¸ ë¶„ì„\n",
    "\n",
    "- **ì˜ˆì‹œ:** `\"ìì—°ì–´ì²˜ë¦¬ëŠ” ì¬ë¯¸ìˆë‹¤\"` â†’ `[\"ìì—°ì–´\", \"ì²˜ë¦¬\", \"ëŠ”\", \"ì¬ë¯¸\", \"ìˆ\", \"ë‹¤\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kiwipiepy import Kiwi\n",
    "import time\n",
    "from functools import wraps\n",
    "import warnings\n",
    "\n",
    "# ë¶ˆí•„ìš”í•œ ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def measure_time(func):\n",
    "    \"\"\"ì‹¤í–‰ ì‹œê°„ ì¸¡ì • ë°ì½”ë ˆì´í„°\"\"\"\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        print(f\"{func.__name__} ì‹¤í–‰ ì‹œê°„: {end-start:.4f}ì´ˆ\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "# Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ë¡œë“œ\n",
    "kiwi = Kiwi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenize_with_kiwi ì‹¤í–‰ ì‹œê°„: 6.4629ì´ˆ\n",
      "í† í°í™” ê²°ê³¼:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Token(form='ìì—°ì–´ ì²˜ë¦¬', tag='NNP', start=0, len=5),\n",
       " Token(form='ë¥¼', tag='JKO', start=5, len=1),\n",
       " Token(form='ê³µë¶€', tag='NNG', start=7, len=2),\n",
       " Token(form='í•˜', tag='XSV', start=9, len=1),\n",
       " Token(form='ëŠ”', tag='ETM', start=10, len=1),\n",
       " Token(form='ê²ƒ', tag='NNB', start=12, len=1),\n",
       " Token(form='ì€', tag='JX', start=13, len=1),\n",
       " Token(form='ì •ë§', tag='MAG', start=15, len=2),\n",
       " Token(form='í¥ë¯¸', tag='NNG', start=18, len=2),\n",
       " Token(form='ë¡­', tag='XSA-I', start=20, len=1),\n",
       " Token(form='ê³ ', tag='EC', start=21, len=1),\n",
       " Token(form='ìœ ìš©', tag='NNG', start=23, len=2),\n",
       " Token(form='í•˜', tag='XSA', start=25, len=1),\n",
       " Token(form='á†¸ë‹ˆë‹¤', tag='EF', start=25, len=3),\n",
       " Token(form='!', tag='SF', start=28, len=1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@measure_time\n",
    "def tokenize_with_kiwi(text):\n",
    "    \"\"\"í† í°í™” í•¨ìˆ˜\"\"\"\n",
    "    try:\n",
    "        return kiwi.tokenize(text)\n",
    "    except Exception as e:\n",
    "        print(f\"í† í°í™” ì˜¤ë¥˜: {e}\")\n",
    "        return []\n",
    "    \n",
    "# í† í°í™” ì‹¤í–‰\n",
    "text = \"ìì—°ì–´ì²˜ë¦¬ë¥¼ ê³µë¶€í•˜ëŠ” ê²ƒì€ ì •ë§ í¥ë¯¸ë¡­ê³  ìœ ìš©í•©ë‹ˆë‹¤!\"\n",
    "tokens = tokenize_with_kiwi(text)\n",
    "\n",
    "print(\"í† í°í™” ê²°ê³¼:\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ë²ˆí˜¸    ë‹¨ì–´      í’ˆì‚¬íƒœê·¸      í•œêµ­ì–´í’ˆì‚¬       ìœ„ì¹˜  \n",
      "---------------------------------------------\n",
      "  1  ìì—°ì–´ ì²˜ë¦¬    NNP        ê³ ìœ ëª…ì‚¬      0-5  \n",
      "  2    ë¥¼       JKO       ëª©ì ê²©ì¡°ì‚¬      5-6  \n",
      "  3    ê³µë¶€      NNG        ì¼ë°˜ëª…ì‚¬      7-9  \n",
      "  4    í•˜       XSV      ë™ì‚¬íŒŒìƒì ‘ë¯¸ì‚¬     9-10 \n",
      "  5    ëŠ”       ETM      ê´€í˜•ì‚¬í˜•ì „ì„±ì–´ë¯¸   10-11 \n",
      "  6    ê²ƒ       NNB        ì˜ì¡´ëª…ì‚¬     12-13 \n",
      "  7    ì€        JX        ë³´ì¡°ì‚¬      13-14 \n",
      "  8    ì •ë§      MAG        ì¼ë°˜ë¶€ì‚¬     15-17 \n",
      "  9    í¥ë¯¸      NNG        ì¼ë°˜ëª…ì‚¬     18-20 \n",
      " 10    ë¡­      XSA-I   í˜•ìš©ì‚¬íŒŒìƒì ‘ë¯¸ì‚¬(ë¶ˆê·œì¹™) 20-21 \n",
      " 11    ê³         EC        ì—°ê²°ì–´ë¯¸     21-22 \n",
      " 12    ìœ ìš©      NNG        ì¼ë°˜ëª…ì‚¬     23-25 \n",
      " 13    í•˜       XSA      í˜•ìš©ì‚¬íŒŒìƒì ‘ë¯¸ì‚¬   25-26 \n",
      " 14   á†¸ë‹ˆë‹¤       EF        ì¢…ê²°ì–´ë¯¸     25-28 \n",
      " 15    !        SF        ë§ˆì¹¨í‘œë¥˜     28-29 \n"
     ]
    }
   ],
   "source": [
    "# í’ˆì‚¬ íƒœê·¸ë¥¼ í•œêµ­ì–´ ëª…ì¹­ìœ¼ë¡œ ë§¤í•‘í•˜ëŠ” ë”•ì…”ë„ˆë¦¬\n",
    "pos_dict = {\n",
    "    # ì²´ì–¸\n",
    "    'NNG': 'ì¼ë°˜ëª…ì‚¬', 'NNP': 'ê³ ìœ ëª…ì‚¬', 'NNB': 'ì˜ì¡´ëª…ì‚¬', 'NP': 'ëŒ€ëª…ì‚¬', 'NR': 'ìˆ˜ì‚¬',\n",
    "    # ìš©ì–¸\n",
    "    'VV': 'ë™ì‚¬', 'VA': 'í˜•ìš©ì‚¬', 'VX': 'ë³´ì¡°ìš©ì–¸', 'VCP': 'ê¸ì •ì§€ì •ì‚¬', 'VCN': 'ë¶€ì •ì§€ì •ì‚¬',\n",
    "    # ê´€í˜•ì‚¬/ë¶€ì‚¬\n",
    "    'MM': 'ê´€í˜•ì‚¬', 'MAG': 'ì¼ë°˜ë¶€ì‚¬', 'MAJ': 'ì ‘ì†ë¶€ì‚¬',\n",
    "    # ì¡°ì‚¬\n",
    "    'JKS': 'ì£¼ê²©ì¡°ì‚¬', 'JKC': 'ë³´ê²©ì¡°ì‚¬', 'JKG': 'ê´€í˜•ê²©ì¡°ì‚¬', 'JKO': 'ëª©ì ê²©ì¡°ì‚¬', \n",
    "    'JKB': 'ë¶€ì‚¬ê²©ì¡°ì‚¬', 'JKV': 'í˜¸ê²©ì¡°ì‚¬', 'JKQ': 'ì¸ìš©ê²©ì¡°ì‚¬', 'JX': 'ë³´ì¡°ì‚¬', 'JC': 'ì ‘ì†ì¡°ì‚¬',\n",
    "    # ì–´ë¯¸\n",
    "    'EP': 'ì„ ì–´ë§ì–´ë¯¸', 'EF': 'ì¢…ê²°ì–´ë¯¸', 'EC': 'ì—°ê²°ì–´ë¯¸', 'ETN': 'ëª…ì‚¬í˜•ì „ì„±ì–´ë¯¸', 'ETM': 'ê´€í˜•ì‚¬í˜•ì „ì„±ì–´ë¯¸',\n",
    "    # ì ‘ì‚¬\n",
    "    'XPN': 'ì²´ì–¸ì ‘ë‘ì‚¬', 'XSN': 'ëª…ì‚¬íŒŒìƒì ‘ë¯¸ì‚¬', 'XSV': 'ë™ì‚¬íŒŒìƒì ‘ë¯¸ì‚¬', 'XSA': 'í˜•ìš©ì‚¬íŒŒìƒì ‘ë¯¸ì‚¬', 'XSA-I': 'í˜•ìš©ì‚¬íŒŒìƒì ‘ë¯¸ì‚¬(ë¶ˆê·œì¹™)', 'XR': 'ì–´ê·¼',\n",
    "    # ê¸°í˜¸\n",
    "    'SF': 'ë§ˆì¹¨í‘œë¥˜', 'SP': 'ì‰¼í‘œë¥˜', 'SS': 'ë”°ì˜´í‘œë¥˜', 'SE': 'ì¤„ì„í‘œ', 'SO': 'ë¶™ì„í‘œ',\n",
    "    'SL': 'ì™¸êµ­ì–´', 'SH': 'í•œì', 'SN': 'ìˆ«ì', 'SW': 'ê¸°íƒ€ê¸°í˜¸'\n",
    "}\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥ \n",
    "print(f\"{'ë²ˆí˜¸':>3} {'ë‹¨ì–´':^8} {'í’ˆì‚¬íƒœê·¸':^8} {'í•œêµ­ì–´í’ˆì‚¬':^12} {'ìœ„ì¹˜':^6}\")\n",
    "print(\"-\" * 45)\n",
    "for i, token in enumerate(tokens, 1):\n",
    "    korean_pos = pos_dict.get(token.tag, token.tag)\n",
    "    pos_range = f\"{token.start}-{token.start + token.len}\"\n",
    "    print(f\"{i:>3} {token.form:^8} {token.tag:^8} {korean_pos:^12} {pos_range:^6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 **ì„œë¸Œì›Œë“œ í† í°í™” (Subword Tokenization)**\n",
    "\n",
    "**íŠ¹ì§•:**\n",
    "- í˜•íƒœì†Œë³´ë‹¤ ì‘ì€ ì˜ë¯¸ ë‹¨ìœ„ ì‚¬ìš©\n",
    "- ìì£¼ ë“±ì¥í•˜ëŠ” ë¬¸ìì—´ íŒ¨í„´ì„ í† í°ìœ¼ë¡œ í™œìš©\n",
    "- OOV(Out-of-Vocabulary) ë¬¸ì œ í•´ê²°ì— íš¨ê³¼ì \n",
    "\n",
    "**í™œìš© ë¶„ì•¼:**\n",
    "- ì‹ ì¡°ì–´, ì „ë¬¸ìš©ì–´ ì²˜ë¦¬\n",
    "- ê¸°ê³„ ë²ˆì—­, ì–¸ì–´ ëª¨ë¸ë§\n",
    "- ë‹¤êµ­ì–´ NLP ëª¨ë¸\n",
    "\n",
    "### 1.2.1 ì£¼ìš” ì„œë¸Œì›Œë“œ ì•Œê³ ë¦¬ì¦˜ ìƒì„¸ (Subword Algorithms)\n",
    "\n",
    "\n",
    "#### 1. **BPE (Byte Pair Encoding)**\n",
    "ê°€ì¥ ê¸°ë³¸ì ì¸ ì„œë¸Œì›Œë“œ ë¶„ì ˆ ë°©ì‹ìœ¼ë¡œ, **'ë°ì´í„°ì— ê°€ì¥ ìì£¼ ë“±ì¥í•˜ëŠ” ê¸€ì ìŒ(Pair)'ì„ ìˆœì°¨ì ìœ¼ë¡œ ë³‘í•©**í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.\n",
    "\n",
    "* **ì‘ë™ ì›ë¦¬ (Bottom-up):**\n",
    "    1. ëª¨ë“  ë‹¨ì–´ë¥¼ ë¬¸ì(Character) ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤.\n",
    "    2. ê°€ì¥ ë¹ˆë„ìˆ˜ê°€ ë†’ì€ ë¬¸ì ìŒ(Bi-gram)ì„ ì°¾ìŠµë‹ˆë‹¤.\n",
    "    3. ì´ ìŒì„ í•˜ë‚˜ì˜ í† í°ìœ¼ë¡œ ë³‘í•©í•˜ê³ , ë‹¨ì–´ ëª©ë¡ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.\n",
    "    4. ì„¤ì •ëœ ì–´íœ˜ ì§‘í•©(Vocab) í¬ê¸°ì— ë„ë‹¬í•  ë•Œê¹Œì§€ 2~3ë²ˆ ê³¼ì •ì„ ë°˜ë³µí•©ë‹ˆë‹¤.\n",
    "\n",
    "* ** ì˜ˆì‹œ:**\n",
    "    * **ì´ˆê¸° ìƒíƒœ:** `l o w` (5íšŒ), `l o w e r` (2íšŒ), `n e w e r` (6íšŒ), `w i d e r` (3íšŒ)\n",
    "    * **1íšŒì°¨ ë³‘í•©:** ê°€ì¥ ìì£¼ ë“±ì¥í•˜ëŠ” `e`ì™€ `r`ì˜ ìŒ(`er`)ì„ ë³‘í•©\n",
    "        â†’ `l o w`, `l o w er`, `n e w er`, `w i d er`\n",
    "    * **ìµœì¢… ê²°ê³¼:** 'low', 'er', 'new', 'wid' ë“±ì˜ ì„œë¸Œì›Œë“œê°€ ìƒì„±ë˜ì–´, **`lower`** ë¼ëŠ” ë‹¨ì–´ëŠ” `low` + `er`ë¡œ í† í°í™”ë©ë‹ˆë‹¤.\n",
    "\n",
    "#### 2. **WordPiece**\n",
    "Googleì´ BERTë¥¼ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•´ BPEë¥¼ ê°œì„ í•œ ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤. BPEì™€ ìœ ì‚¬í•˜ì§€ë§Œ ë³‘í•© ê¸°ì¤€ì´ ë‹¨ìˆœíˆ 'ë¹ˆë„'ê°€ ì•„ë‹ˆë¼ **'ìš°ë„(Likelihood)'ë¥¼ ê°€ì¥ ë†’ì´ëŠ” ìŒ**ì„ ì„ íƒí•©ë‹ˆë‹¤.\n",
    "\n",
    "* **ì‘ë™ ì›ë¦¬:**\n",
    "    * ë¹ˆë„ìˆ˜ë§Œ ê³ ë ¤í•˜ë©´ ìì£¼ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ë¼ë¦¬ë§Œ ë­‰ì³ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "    * WordPieceëŠ” ì•„ë˜ ê³µì‹ì„ ì‚¬ìš©í•˜ì—¬, ë‘ ê¸€ìê°€ ë”°ë¡œ ìˆì„ ë•Œë³´ë‹¤ í•©ì³ì¡Œì„ ë•Œì˜ ì¤‘ìš”ë„ê°€ ë†’ì€ ìŒì„ ë¨¼ì € ë³‘í•©í•©ë‹ˆë‹¤.\n",
    "    * **ìˆ˜ì‹:** $Score = \\frac{Frequency(pair)}{Frequency(first) \\times Frequency(second)}$\n",
    "    * **ì ‘ë‘ì–´ í‘œì‹œ:** ë‹¨ì–´ì˜ ì²« ë¶€ë¶„ì´ ì•„ë‹Œ ë‚˜ë¨¸ì§€ ë¶€ë¶„(ì¤‘ê°„, ë)ì—ëŠ” `##`ì„ ë¶™ì—¬ êµ¬ë¶„í•©ë‹ˆë‹¤.\n",
    "\n",
    "* **ì˜ˆì‹œ (BERT Tokenizer):**\n",
    "    * ì…ë ¥ ë‹¨ì–´: **`embeddings`**\n",
    "    * í† í°í™” ê²°ê³¼: `['em', '##bed', '##ding', '##s']`\n",
    "    * **í•´ì„¤:** `em`ì€ ë‹¨ì–´ì˜ ì‹œì‘ì„. `##bed`ëŠ” ì•ì— ë‹¤ë¥¸ ê¸€ìê°€ ë¶™ì–´ ìˆë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.\n",
    "\n",
    "#### 3. **SentencePiece**\n",
    "Googleì´ ë§Œë“  ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ, **'ê³µë°±(Space)'ë„ í•˜ë‚˜ì˜ ë¬¸ìë¡œ ì·¨ê¸‰**í•˜ëŠ” ê²ƒì´ ê°€ì¥ í° íŠ¹ì§•ì…ë‹ˆë‹¤.\n",
    "\n",
    " **ì‘ë™ ì›ë¦¬:**\n",
    "    * ê¸°ì¡´ BPEë‚˜ WordPieceëŠ” ë„ì–´ì“°ê¸°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¨¼ì € ë‹¨ì–´ë¥¼ ë‚˜ëˆˆ ë’¤(Pre-tokenization) ì„œë¸Œì›Œë“œë¡œ ìª¼ê°œì§€ë§Œ, SentencePieceëŠ” ë¬¸ì¥ ì „ì²´ë¥¼ ê·¸ëƒ¥ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ë´…ë‹ˆë‹¤.\n",
    "    * ê³µë°±ì„ ì–¸ë”ë°”(`_`, U+2581)ë¡œ ì¹˜í™˜í•˜ì—¬ ì²˜ë¦¬í•˜ë¯€ë¡œ, ë„ì–´ì“°ê¸°ê°€ ì—†ëŠ” ì–¸ì–´(ì¤‘êµ­ì–´, ì¼ë³¸ì–´)ë‚˜ ë„ì–´ì“°ê¸° ê·œì¹™ì´ ë³µì¡í•œ ì–¸ì–´(í•œêµ­ì–´)ì— ë§¤ìš° ê°•ë ¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "* **ì˜ˆì‹œ:**\n",
    "    * ì…ë ¥ ë¬¸ì¥: **`Hello World`**\n",
    "    * í† í°í™” ê²°ê³¼: `['_Hello', '_World']`\n",
    "    * **ë³µì›(Detokenization):** ë‹¨ìˆœíˆ ëª¨ë“  í† í°ì„ í•©ì¹œ ë’¤ `_`ë¥¼ ê³µë°±ìœ¼ë¡œ ë°”ê¾¸ê¸°ë§Œ í•˜ë©´ ì›ë¬¸ì´ ì™„ë²½í•˜ê²Œ ë³µì›ë©ë‹ˆë‹¤.\n",
    "\n",
    "\n",
    "**ì£¼ìš” ì•Œê³ ë¦¬ì¦˜ ë¹„êµ:**\n",
    "\n",
    "| êµ¬ë¶„ | BPE | WordPiece | SentencePiece |\n",
    "| --- | --- | --- | --- |\n",
    "| **ë³‘í•© ê¸°ì¤€** | **ë¹ˆë„ìˆ˜ (Frequency)** | **ìš°ë„ (Likelihood)** | **BPE ë˜ëŠ” Unigram** |\n",
    "| **íŠ¹ìˆ˜ ê¸°í˜¸** | `</w>` (ë‹¨ì–´ ë) ë“± | `##` (ë‹¨ì–´ì˜ ì¤‘ê°„/ë) | `_` (ê³µë°±) |\n",
    "| **ì „ì²˜ë¦¬** | ë„ì–´ì“°ê¸° ê¸°ì¤€ ë¶„ë¦¬ í•„ìš” | ë„ì–´ì“°ê¸° ê¸°ì¤€ ë¶„ë¦¬ í•„ìš” | **í•„ìš” ì—†ìŒ (Raw Text ì…ë ¥)** |\n",
    "| **ì£¼ìš” ëª¨ë¸** | GPT-2, GPT-3, RoBERTa | BERT, DistilBERT | T5, XLM-R, ALBERT |\n",
    "| **ì í•©ì„±** | ì˜ì–´ ë“± ë„ì–´ì“°ê¸° ëª…í™•í•œ ì–¸ì–´ | ì˜ì–´, ì˜ë¯¸ë¡ ì  ë¶„ì„ ì¤‘ìš” ì‹œ | **í•œêµ­ì–´, ë‹¤êµ­ì–´ ì²˜ë¦¬** |\n",
    "\n",
    "\n",
    "### 1.2.2 Byte-Level BPE (BBPE)\n",
    "\n",
    "ìµœì‹  LLM(GPT ì‹œë¦¬ì¦ˆ, LLaMA)ì—ì„œ í‘œì¤€ì²˜ëŸ¼ ì‚¬ìš©ë˜ëŠ” ë°©ì‹ìœ¼ë¡œ, ê¸°ì¡´ BPEì˜ í•œê³„ì¸ OOV ë¬¸ì œë¥¼ ê·¼ë³¸ì ìœ¼ë¡œ í•´ê²°í•œ ë°©ì‹ì…ë‹ˆë‹¤.\n",
    "\n",
    "* **ì‘ë™ ì›ë¦¬:**\n",
    "    1. í…ìŠ¤íŠ¸ë¥¼ ì‚¬ëŒì´ ë³´ëŠ” 'ë¬¸ì(Character)'ê°€ ì•„ë‹Œ ì»´í“¨í„°ê°€ ì²˜ë¦¬í•˜ëŠ” **'UTF-8 ë°”ì´íŠ¸(Byte)'** ë‹¨ìœ„ë¡œ ë¶„í•´í•©ë‹ˆë‹¤.\n",
    "    2. 256ê°œì˜ ë°”ì´íŠ¸ë¥¼ ê¸°ë³¸ ì–´íœ˜ë¡œ ì‹œì‘í•˜ì—¬ BPE ì•Œê³ ë¦¬ì¦˜(ë¹ˆë„ ê¸°ë°˜ ë³‘í•©)ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "* **íŠ¹ì§•:**\n",
    "    * **OOV(Out-Of-Vocabulary) ì™„ì „ í•´ê²°:** ì–´ë–¤ ì–¸ì–´, ì´ëª¨ì§€, íŠ¹ìˆ˜ë¬¸ìê°€ ë“¤ì–´ì™€ë„ ë°”ì´íŠ¸ ë‹¨ìœ„ë¡œ ìª¼ê°œì„œ í‘œí˜„í•˜ë¯€ë¡œ `<UNK>`(Unknown Token)ì´ ë°œìƒí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
    "    * **ë‹¤êµ­ì–´ ì²˜ë¦¬ ìš°ìˆ˜:** ë³„ë„ì˜ ì–¸ì–´ë³„ ì „ì²˜ë¦¬ ì—†ì´ë„ ëª¨ë“  ì–¸ì–´ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "* **ì˜ˆì‹œ:**\n",
    "    * ì…ë ¥: \"AğŸ˜Š\"\n",
    "    * ì²˜ë¦¬: 'A'(1ë°”ì´íŠ¸) + 'ğŸ˜Š'(4ë°”ì´íŠ¸) â†’ ì´ 5ê°œì˜ ë°”ì´íŠ¸ í† í°ìœ¼ë¡œ ì‹œì‘í•´ ìì£¼ ë‚˜ì˜¤ëŠ” íŒ¨í„´ë¼ë¦¬ ë³‘í•©.\n",
    "    * ì‚¬ìš© ëª¨ë¸: **GPT-2, GPT-3, GPT-4, RoBERTa, LLaMA**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **ì‹¤ìŠµ**: BERT í† í¬ë‚˜ì´ì € (WordPiece)\n",
    "\n",
    "- **Huggingface KcBERT í† í¬ë‚˜ì´ì €** í™œìš© \n",
    "- ê°œë…:\n",
    "    - SentencePiece ê¸°ë°˜ ì„œë¸Œì›Œë“œ í† í¬ë‚˜ì´ì € ì‚¬ìš©\n",
    "    - í•œêµ­ì–´ì— íŠ¹í™”ëœ ì–´íœ˜ ì‚¬ì „(vocab) ë³´ìœ \n",
    "    - íŠ¹ìˆ˜ í† í°: [CLS], [SEP], [MASK], [PAD], [UNK]\n",
    "- íŠ¹ì§•:\n",
    "    - ë¬¸ì¥ ì‹œì‘: [CLS] í† í° ì¶”ê°€\n",
    "    - ë¬¸ì¥ êµ¬ë¶„: [SEP] í† í° ì‚¬ìš©\n",
    "    - ì„œë¸Œì›Œë“œ ë¶„ë¦¬: '##' ì ‘ë‘ì–´ë¡œ í‘œì‹œ\n",
    "    - ì˜ˆì‹œ: `\"ìì—°ì–´ì²˜ë¦¬\"` â†’ `['ìì—°', '##ì–´', '##ì²˜ë¦¬']`\n",
    "    - ìµœëŒ€ ê¸¸ì´ ì²˜ë¦¬: max_lengthë¡œ ìë™ truncation \n",
    "    - ì°¸ì¡°: https://huggingface.co/beomi/kcbert-base\n",
    "- ì„¤ì¹˜:\n",
    "    - pip install torch transformers ë˜ëŠ” uv add torch transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Disabling PyTorch because PyTorch >= 2.4 is required but found 2.2.2\n",
      "PyTorch was not found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc033cb572574c69b069116bd158c736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/619 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74349746806d471b9ab94c7d8a24e948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87bd96bc840d4198b181ecd38ed30c86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='beomi/kcbert-base', vocab_size=30000, model_max_length=300, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# í—ˆê¹…í˜ì´ìŠ¤ íŠ¸ëœìŠ¤í¬ë¨¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('beomi/kcbert-base')\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ìì—°', '##ì–´', '##ì²˜ë¦¬', '##ë¥¼', 'ê³µë¶€', '##í•©ë‹ˆë‹¤']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# í† í°í™” ìˆ˜í–‰ : ë¬¸ì¥ -> í† í°ë“¤ì˜ ë¦¬ìŠ¤íŠ¸\n",
    "text = \"ìì—°ì–´ì²˜ë¦¬ë¥¼ ê³µë¶€í•©ë‹ˆë‹¤\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. ì›ë¬¸: ìì—°ì–´ì²˜ë¦¬ë¥¼ ê³µë¶€í•©ë‹ˆë‹¤\n",
      "   í† í° ìˆ˜: 6ê°œ\n",
      "   ì„œë¸Œì›Œë“œ: 4ê°œ\n",
      "   í† í°ë“¤: ['ìì—°', '##ì–´', '##ì²˜ë¦¬', '##ë¥¼', 'ê³µë¶€', '##í•©ë‹ˆë‹¤']\n",
      "------------------------------\n",
      "\n",
      "2. ì›ë¬¸: ì¸ê³µì§€ëŠ¥ê³¼ ë¨¸ì‹ ëŸ¬ë‹ì€ ë¯¸ë˜ê¸°ìˆ ì´ë‹¤\n",
      "   í† í° ìˆ˜: 11ê°œ\n",
      "   ì„œë¸Œì›Œë“œ: 8ê°œ\n",
      "   í† í°ë“¤: ['ì¸ê³µ', '##ì§€ëŠ¥', '##ê³¼', 'ë¨¸', '##ì‹ ', '##ëŸ¬', '##ë‹', '##ì€', 'ë¯¸ë˜', '##ê¸°ìˆ ', '##ì´ë‹¤']\n",
      "------------------------------\n",
      "\n",
      "3. ì›ë¬¸: COVID-19ë¡œ ì¸í•œ ë¹„ëŒ€ë©´ ìˆ˜ì—…ì´ ì¦ê°€í–ˆë‹¤\n",
      "   í† í° ìˆ˜: 15ê°œ\n",
      "   ì„œë¸Œì›Œë“œ: 8ê°œ\n",
      "   í† í°ë“¤: ['C', '##O', '##V', '##I', '##D', '-', '19', '##ë¡œ', 'ì¸í•œ', 'ë¹„', '##ëŒ€ë©´', 'ìˆ˜ì—…', '##ì´', 'ì¦ê°€', '##í–ˆë‹¤']\n",
      "------------------------------\n",
      "\n",
      "4. ì›ë¬¸: ìŠ¤ë§ˆíŠ¸í°ì˜ ìŒì„±ì¸ì‹ ê¸°ëŠ¥ì´ ë°œì „í–ˆë‹¤\n",
      "   í† í° ìˆ˜: 8ê°œ\n",
      "   ì„œë¸Œì›Œë“œ: 4ê°œ\n",
      "   í† í°ë“¤: ['ìŠ¤ë§ˆíŠ¸í°', '##ì˜', 'ìŒì„±', '##ì¸ì‹', 'ê¸°ëŠ¥', '##ì´', 'ë°œì „', '##í–ˆë‹¤']\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def analyze_tokenization(texts, tokenizer):\n",
    "    \"\"\"í† í°í™” ë¶„ì„ í•¨ìˆ˜\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for text in texts:\n",
    "        try:\n",
    "            # í† í°í™”\n",
    "            tokens = tokenizer.tokenize(text)\n",
    "            token_ids = tokenizer.encode(text)\n",
    "            \n",
    "            # íŠ¹ìˆ˜ í† í° ë¶„ì„\n",
    "            special_tokens = [token for token in tokens if token in ['[CLS]', '[SEP]', '[MASK]', '[PAD]', '[UNK]']]\n",
    "            subword_tokens = [token for token in tokens if token.startswith('##')]\n",
    "            \n",
    "            results.append({\n",
    "                'text': text,\n",
    "                'token_count': len(tokens),\n",
    "                'subword_count': len(subword_tokens),\n",
    "                'special_count': len(special_tokens),\n",
    "                'tokens': tokens\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"í† í°í™” ì˜¤ë¥˜ - {text}: {e}\")\n",
    "            \n",
    "    return results\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë¬¸ì¥ë“¤\n",
    "test_texts = [\n",
    "    \"ìì—°ì–´ì²˜ë¦¬ë¥¼ ê³µë¶€í•©ë‹ˆë‹¤\",\n",
    "    \"ì¸ê³µì§€ëŠ¥ê³¼ ë¨¸ì‹ ëŸ¬ë‹ì€ ë¯¸ë˜ê¸°ìˆ ì´ë‹¤\", \n",
    "    \"COVID-19ë¡œ ì¸í•œ ë¹„ëŒ€ë©´ ìˆ˜ì—…ì´ ì¦ê°€í–ˆë‹¤\",\n",
    "    \"ìŠ¤ë§ˆíŠ¸í°ì˜ ìŒì„±ì¸ì‹ ê¸°ëŠ¥ì´ ë°œì „í–ˆë‹¤\"\n",
    "]\n",
    "\n",
    "# í† í°í™” ë¶„ì„\n",
    "results = analyze_tokenization(test_texts, tokenizer)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. ì›ë¬¸: {result['text']}\")\n",
    "    print(f\"   í† í° ìˆ˜: {result['token_count']}ê°œ\")\n",
    "    print(f\"   ì„œë¸Œì›Œë“œ: {result['subword_count']}ê°œ\")\n",
    "    print(f\"   í† í°ë“¤: {result['tokens']}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **ì‹¤ìŠµ**: SentencePiece í† í¬ë‚˜ì´ì € (bge-m3 ëª¨ë¸)\n",
    "\n",
    "- **Huggingface Transformers** ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24d115ccc04240f8ba21652426a68e9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/687 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f03078546bd4457c8f674b12fda34d99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/444 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ec971c046284d7687f28c94219be247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e22099db0f444afbf431d0f82553419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "392bebdb77d84ae190c529949e55e52b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-m3\") # ì˜¤í”ˆì†ŒìŠ¤ ì¤‘ í•œêµ­ì–´ ì˜ ì²˜ë¦¬í•˜ëŠ” ì¢‹ì€ ëª¨ë¸, ì¤‘êµ­ êº¼, ë„¤ì´ë²„ í•˜ì´í¼?ì—ì„œë„ ì œê³µí•¨\n",
    "# model = AutoModel.from_pretrained(\"BAAI/bge-m3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â–ìì—°', 'ì–´', 'ì²˜ë¦¬', 'ë¥¼', 'â–ê³µë¶€', 'í•©ë‹ˆë‹¤']\n"
     ]
    }
   ],
   "source": [
    "text = \"ìì—°ì–´ì²˜ë¦¬ë¥¼ ê³µë¶€í•©ë‹ˆë‹¤\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. ì›ë¬¸: ìì—°ì–´ì²˜ë¦¬ë¥¼ ê³µë¶€í•©ë‹ˆë‹¤\n",
      "   í† í° ìˆ˜: 6ê°œ\n",
      "   í† í°ë“¤: ['â–ìì—°', 'ì–´', 'ì²˜ë¦¬', 'ë¥¼', 'â–ê³µë¶€', 'í•©ë‹ˆë‹¤']\n",
      "------------------------------\n",
      "\n",
      "2. ì›ë¬¸: ì¸ê³µì§€ëŠ¥ê³¼ ë¨¸ì‹ ëŸ¬ë‹ì€ ë¯¸ë˜ê¸°ìˆ ì´ë‹¤\n",
      "   í† í° ìˆ˜: 10ê°œ\n",
      "   í† í°ë“¤: ['â–ì¸ê³µì§€ëŠ¥', 'ê³¼', 'â–ë¨¸', 'ì‹ ', 'ëŸ¬', 'ë‹', 'ì€', 'â–ë¯¸ë˜', 'ê¸°ìˆ ', 'ì´ë‹¤']\n",
      "------------------------------\n",
      "\n",
      "3. ì›ë¬¸: COVID-19ë¡œ ì¸í•œ ë¹„ëŒ€ë©´ ìˆ˜ì—…ì´ ì¦ê°€í–ˆë‹¤\n",
      "   í† í° ìˆ˜: 12ê°œ\n",
      "   í† í°ë“¤: ['â–CO', 'VID', '-19', 'ë¡œ', 'â–ì¸í•œ', 'â–ë¹„', 'ëŒ€', 'ë©´', 'â–ìˆ˜ì—…', 'ì´', 'â–ì¦ê°€', 'í–ˆë‹¤']\n",
      "------------------------------\n",
      "\n",
      "4. ì›ë¬¸: ìŠ¤ë§ˆíŠ¸í°ì˜ ìŒì„±ì¸ì‹ ê¸°ëŠ¥ì´ ë°œì „í–ˆë‹¤\n",
      "   í† í° ìˆ˜: 9ê°œ\n",
      "   í† í°ë“¤: ['â–ìŠ¤ë§ˆíŠ¸í°', 'ì˜', 'â–ìŒì„±', 'ì¸', 'ì‹', 'â–ê¸°ëŠ¥', 'ì´', 'â–ë°œì „', 'í–ˆë‹¤']\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# í† í°í™” ë¶„ì„\n",
    "results = analyze_tokenization(test_texts, tokenizer)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. ì›ë¬¸: {result['text']}\")\n",
    "    print(f\"   í† í° ìˆ˜: {result['token_count']}ê°œ\")\n",
    "    print(f\"   í† í°ë“¤: {result['tokens']}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Embedding (ì„ë² ë”©)\n",
    "\n",
    "í…ìŠ¤íŠ¸ë¥¼ ì»´í“¨í„°ê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” ìˆ˜ì¹˜ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ê¸°ìˆ ì´ë‹¤. ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ í…ìŠ¤íŠ¸ëŠ” ë²¡í„° ê³µê°„ì—ì„œ ê°€ê¹Œìš´ ê±°ë¦¬ì— ìœ„ì¹˜í•˜ë„ë¡ í•œë‹¤.\n",
    "\n",
    "### 2.1 **ë‹¨ì–´ ì„ë² ë”© (Word Embedding)**\n",
    "\n",
    "- ê°œë…:\n",
    "    - ìì—°ì–´ë¥¼ ì»´í“¨í„°ê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” ë²¡í„° í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” ê¸°ìˆ \n",
    "    - ê° ë‹¨ì–´ë¥¼ ê³ ì •ëœ í¬ê¸°ì˜ ì‹¤ìˆ˜ ë²¡í„°ë¡œ í‘œí˜„\n",
    "    - ë¹„ìŠ·í•œ ì˜ë¯¸ë¥¼ ê°€ì§„ ë‹¨ì–´ë“¤ì€ ë²¡í„° ê³µê°„ì—ì„œ ì„œë¡œ ê°€ê¹Œìš´ ê±°ë¦¬ì— ìœ„ì¹˜\n",
    "\n",
    "- ê¸°ë²•: \n",
    "    - Bag of Words (BoW)\n",
    "    - TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "    - Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Bag of Words (BoW)\n",
    "\n",
    "**íŠ¹ì§•:**\n",
    "- ë‹¨ì–´ì˜ ì¶œí˜„ ë¹ˆë„ë¥¼ ë²¡í„°ë¡œ í‘œí˜„\n",
    "- ë‹¨ì–´ ìˆœì„œ ì •ë³´ëŠ” ë¬´ì‹œ\n",
    "- í¬ì†Œ(sparse) ë²¡í„° ìƒì„±\n",
    "\n",
    "* uv add scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenize_with_kiwi ì‹¤í–‰ ì‹œê°„: 0.0058ì´ˆ\n",
      "tokenize_with_kiwi ì‹¤í–‰ ì‹œê°„: 0.0078ì´ˆ\n",
      "tokenize_with_kiwi ì‹¤í–‰ ì‹œê°„: 0.0046ì´ˆ\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì˜ˆì œ\n",
    "texts = [\n",
    "    \"ìì—°ì–´ ì²˜ë¦¬ë¥¼ ê³µë¶€í•©ë‹ˆë‹¤\",\n",
    "    \"ìì—°ì–´ëŠ” ì»´í“¨í„° ì–¸ì–´ê°€ ì•„ë‹ˆë¼ ì¸ê°„ì˜ ì–¸ì–´ì…ë‹ˆë‹¤\", \n",
    "    \"ìì—°ì–´ ìˆ˜ì—… ì‹œê°„ì— ìì—°ì–´ ì²˜ë¦¬ ë°©ë²•ì„ ë°°ìš°ê³  ìˆìŠµë‹ˆë‹¤\"\n",
    "]\n",
    "\n",
    "# BoW ë²¡í„°í™”\n",
    "def create_bow_vectors(texts):\n",
    "    # í† í°í™”ëœ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "    tokenized_texts = []\n",
    "    for text in texts:\n",
    "        tokens = tokenize_with_kiwi(text)\n",
    "        token_words = [token.form for token in tokens if token.tag not in ['SF', 'SP']]  # êµ¬ë‘ì  ì œì™¸\n",
    "        tokenized_texts.append(' '.join(token_words))\n",
    "    \n",
    "    # BoW ë²¡í„°í™”\n",
    "    vectorizer = CountVectorizer()\n",
    "    bow_matrix = vectorizer.fit_transform(tokenized_texts)\n",
    "    \n",
    "    return bow_matrix, vectorizer, tokenized_texts\n",
    "\n",
    "bow_matrix, vectorizer, tokenized_texts = create_bow_vectors(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>á†¸ë‹ˆë‹¤</th>\n",
       "      <th>ê³µë¶€</th>\n",
       "      <th>ë°©ë²•</th>\n",
       "      <th>ë°°ìš°</th>\n",
       "      <th>ìˆ˜ì—…</th>\n",
       "      <th>ìŠµë‹ˆë‹¤</th>\n",
       "      <th>ì‹œê°„</th>\n",
       "      <th>ì•„ë‹ˆ</th>\n",
       "      <th>ì–¸ì–´</th>\n",
       "      <th>ì—°ì–´</th>\n",
       "      <th>ì¸ê°„</th>\n",
       "      <th>ìì—°ì–´</th>\n",
       "      <th>ì²˜ë¦¬</th>\n",
       "      <th>ì»´í“¨í„°</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ë¬¸ì„œ1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ë¬¸ì„œ2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ë¬¸ì„œ3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     á†¸ë‹ˆë‹¤  ê³µë¶€  ë°©ë²•  ë°°ìš°  ìˆ˜ì—…  ìŠµë‹ˆë‹¤  ì‹œê°„  ì•„ë‹ˆ  ì–¸ì–´  ì—°ì–´  ì¸ê°„  ìì—°ì–´  ì²˜ë¦¬  ì»´í“¨í„°\n",
       "ë¬¸ì„œ1    1   1   0   0   0    0   0   0   0   0   0    1   1    0\n",
       "ë¬¸ì„œ2    1   0   0   0   0    0   0   1   2   1   1    0   0    1\n",
       "ë¬¸ì„œ3    0   0   1   1   1    1   1   0   0   0   0    2   1    0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ê²°ê³¼ ì •ë¦¬\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "bow_df = pd.DataFrame(bow_matrix.toarray(), \n",
    "                     columns=feature_names,\n",
    "                     index=[f'ë¬¸ì„œ{i+1}' for i in range(len(texts))])\n",
    "\n",
    "bow_df # ê²°ê³¼ ì¤‘ ê°’ì´ 0ì¸ ê²ƒ = sparse(í¬ì†Œ) vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì–´íœ˜ ì‚¬ì „ í¬ê¸°: 14ê°œ\n"
     ]
    }
   ],
   "source": [
    "print(f\"ì–´íœ˜ ì‚¬ì „ í¬ê¸°: {len(feature_names)}ê°œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "**íŠ¹ì§•:**\n",
    "- ë‹¨ì–´ì˜ ì¤‘ìš”ë„ë¥¼ ê³ ë ¤í•œ ê°€ì¤‘ì¹˜ ë¶€ì—¬\n",
    "- í¬ê·€í•œ ë‹¨ì–´ì— ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬\n",
    "- ë¬¸ì„œ ì§‘í•© ë‚´ì—ì„œ ë‹¨ì–´ì˜ ìƒëŒ€ì  ì¤‘ìš”ì„± ì¸¡ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenize_with_kiwi ì‹¤í–‰ ì‹œê°„: 0.0186ì´ˆ\n",
      "tokenize_with_kiwi ì‹¤í–‰ ì‹œê°„: 0.0033ì´ˆ\n",
      "tokenize_with_kiwi ì‹¤í–‰ ì‹œê°„: 0.0022ì´ˆ\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def create_tfidf_vectors(texts):\n",
    "    # í† í°í™”\n",
    "    tokenized_texts = []\n",
    "    for text in texts:\n",
    "        tokens = tokenize_with_kiwi(text)\n",
    "        token_words = [token.form for token in tokens if token.tag not in ['SF', 'SP']]\n",
    "        tokenized_texts.append(' '.join(token_words))\n",
    "    \n",
    "    # TF-IDF ë²¡í„°í™”\n",
    "    tfidf = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf.fit_transform(tokenized_texts)\n",
    "    \n",
    "    return tfidf_matrix, tfidf, tokenized_texts\n",
    "\n",
    "tfidf_matrix, tfidf, _ = create_tfidf_vectors(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>á†¸ë‹ˆë‹¤</th>\n",
       "      <th>ê³µë¶€</th>\n",
       "      <th>ë°©ë²•</th>\n",
       "      <th>ë°°ìš°</th>\n",
       "      <th>ìˆ˜ì—…</th>\n",
       "      <th>ìŠµë‹ˆë‹¤</th>\n",
       "      <th>ì‹œê°„</th>\n",
       "      <th>ì•„ë‹ˆ</th>\n",
       "      <th>ì–¸ì–´</th>\n",
       "      <th>ì—°ì–´</th>\n",
       "      <th>ì¸ê°„</th>\n",
       "      <th>ìì—°ì–´</th>\n",
       "      <th>ì²˜ë¦¬</th>\n",
       "      <th>ì»´í“¨í„°</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ë¬¸ì„œ1</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.605</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ë¬¸ì„œ2</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ë¬¸ì„œ3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.356</td>\n",
       "      <td>0.356</td>\n",
       "      <td>0.356</td>\n",
       "      <td>0.356</td>\n",
       "      <td>0.356</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.541</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      á†¸ë‹ˆë‹¤     ê³µë¶€     ë°©ë²•     ë°°ìš°     ìˆ˜ì—…    ìŠµë‹ˆë‹¤     ì‹œê°„     ì•„ë‹ˆ     ì–¸ì–´     ì—°ì–´  \\\n",
       "ë¬¸ì„œ1  0.46  0.605  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000   \n",
       "ë¬¸ì„œ2  0.26  0.000  0.000  0.000  0.000  0.000  0.000  0.341  0.683  0.341   \n",
       "ë¬¸ì„œ3  0.00  0.000  0.356  0.356  0.356  0.356  0.356  0.000  0.000  0.000   \n",
       "\n",
       "        ì¸ê°„    ìì—°ì–´     ì²˜ë¦¬    ì»´í“¨í„°  \n",
       "ë¬¸ì„œ1  0.000  0.460  0.460  0.000  \n",
       "ë¬¸ì„œ2  0.341  0.000  0.000  0.341  \n",
       "ë¬¸ì„œ3  0.000  0.541  0.271  0.000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ê²°ê³¼ ì •ë¦¬\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), \n",
    "                       columns=feature_names,\n",
    "                       index=[f'ë¬¸ì„œ{i+1}' for i in range(len(texts))])\n",
    "\n",
    "tfidf_df.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 Word2Vec\n",
    "\n",
    "**íŠ¹ì§•:**\n",
    "- ë‹¨ì–´ì˜ ì˜ë¯¸ì  ê´€ê³„ë¥¼ ë²¡í„° ê³µê°„ì— í‘œí˜„\n",
    "- ë¬¸ë§¥ì„ ê³ ë ¤í•œ ë‹¨ì–´ í‘œí˜„ e.g. êµ­ê°€-ìˆ˜ë„ ê´€ê³„, ë‚¨/ì—¬ì„± ê´€ê³„ ë“±\n",
    "- CBOWì™€ Skip-gram ë‘ ê°€ì§€ ì•„í‚¤í…ì²˜\n",
    "- ì„ë² ë”© í”„ë¡œì í„°: https://projector.tensorflow.org/\n",
    "- ì°¸ì¡°: https://ko.wikipedia.org/wiki/Gensim\n",
    "- ì„¤ì¹˜: pip install gensim ë˜ëŠ” uv add gensim\n",
    "\n",
    "ë²¡í„° = ë°©í–¥ ê°€ì§, ê³„ì‚° ê°€ëŠ¥ -> ___(2026.2.10(í™”) 20h55~21h ì„¤ëª… ì œëŒ€ë¡œ ëª» ë“¤ìŒ ã… ã… )\n",
    "\n",
    "**ì£¼ì˜ì‚¬í•­:**\n",
    "- Word2Vecì€ ëŒ€ëŸ‰ì˜ í…ìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ í•™ìŠµí•  ë•Œ íš¨ê³¼ì ì…ë‹ˆë‹¤\n",
    "- ì•„ë˜ ì˜ˆì œëŠ” 5ê°œì˜ ì§§ì€ ë¬¸ì¥ë§Œìœ¼ë¡œ í•™ìŠµí•˜ë¯€ë¡œ ìœ ì˜ë¯¸í•œ ë‹¨ì–´ ë²¡í„°ë¥¼ ì–»ê¸° ì–´ë µìŠµë‹ˆë‹¤\n",
    "- ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œëŠ” ìµœì†Œ ìˆ˜ì²œ ê°œ ì´ìƒì˜ ë¬¸ì¥ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ê¶Œì¥ë©ë‹ˆë‹¤\n",
    "- ì´ ì˜ˆì œëŠ” Word2Vecì˜ ë™ì‘ ì›ë¦¬ë¥¼ ì´í•´í•˜ê¸° ìœ„í•œ êµìœ¡ ëª©ì ì…ë‹ˆë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def train_word2vec_model(texts, vector_size=100, window=3, min_count=1):\n",
    "    \"\"\"Word2Vec ëª¨ë¸ í›ˆë ¨\"\"\"\n",
    "    # í† í°í™”\n",
    "    tokenized_corpus = []\n",
    "    for text in texts:\n",
    "        tokens = tokenize_with_kiwi(text)\n",
    "        token_words = [token.form for token in tokens if len(token.form) > 1]  # í•œ ê¸€ì ë‹¨ì–´ ì œì™¸\n",
    "        tokenized_corpus.append(token_words)\n",
    "    \n",
    "    # ëª¨ë¸ í›ˆë ¨ (Skip-gram)\n",
    "    model = Word2Vec(\n",
    "        sentences=tokenized_corpus,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        workers=4,\n",
    "        sg=1,  # 1: Skip-gram, 0: CBOW\n",
    "        epochs=100\n",
    "    )\n",
    "    \n",
    "    return model, tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenize_with_kiwi ì‹¤í–‰ ì‹œê°„: 0.1636ì´ˆ\n",
      "tokenize_with_kiwi ì‹¤í–‰ ì‹œê°„: 0.5077ì´ˆ\n",
      "tokenize_with_kiwi ì‹¤í–‰ ì‹œê°„: 0.0649ì´ˆ\n",
      "tokenize_with_kiwi ì‹¤í–‰ ì‹œê°„: 0.1378ì´ˆ\n",
      "tokenize_with_kiwi ì‹¤í–‰ ì‹œê°„: 0.2023ì´ˆ\n",
      "ì–´íœ˜ ì‚¬ì „ í¬ê¸°: 24ê°œ\n",
      "ë²¡í„° ì°¨ì›: 100ì°¨ì›\n"
     ]
    }
   ],
   "source": [
    "# ë” í° ì½”í¼ìŠ¤ë¡œ í…ŒìŠ¤íŠ¸\n",
    "word2vec_texts = [\n",
    "    \"ìì—°ì–´ ì²˜ë¦¬ëŠ” ì¸ê³µì§€ëŠ¥ì˜ ì¤‘ìš”í•œ ë¶„ì•¼ì…ë‹ˆë‹¤\",\n",
    "    \"ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì´ ìì—°ì–´ ì²˜ë¦¬ì— í™œìš©ë©ë‹ˆë‹¤\",\n",
    "    \"ì»´í“¨í„°ê°€ ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì´í•´í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤\",\n",
    "    \"í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤\",\n",
    "    \"ë‹¨ì–´ì˜ ì˜ë¯¸ì™€ ë¬¸ë§¥ì„ íŒŒì•…í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤\"\n",
    "]\n",
    "\n",
    "word2vec_model, tokenized_corpus = train_word2vec_model(word2vec_texts)\n",
    "\n",
    "print(f\"ì–´íœ˜ ì‚¬ì „ í¬ê¸°: {len(word2vec_model.wv.key_to_index)}ê°œ\")\n",
    "print(f\"ë²¡í„° ì°¨ì›: {word2vec_model.vector_size}ì°¨ì›\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ìì—°ì–´ ì²˜ë¦¬', 'ì¸ê³µ', 'ì§€ëŠ¥', 'ì¤‘ìš”', 'ë¶„ì•¼', 'á†¸ë‹ˆë‹¤'],\n",
       " ['ë¨¸ì‹ ', 'ëŸ¬ë‹', 'ëŸ¬ë‹', 'ìì—°ì–´ ì²˜ë¦¬', 'í™œìš©', 'á†¸ë‹ˆë‹¤'],\n",
       " ['ì»´í“¨í„°', 'ì¸ê°„', 'ì–¸ì–´', 'ì´í•´', 'ê¸°ìˆ ', 'á†¸ë‹ˆë‹¤'],\n",
       " ['í…ìŠ¤íŠ¸', 'ë°ì´í„°', 'ë¶„ì„', 'ì²˜ë¦¬', 'ë°©ë²•', 'í•™ìŠµ', 'á†¸ë‹ˆë‹¤'],\n",
       " ['ë‹¨ì–´', 'ì˜ë¯¸', 'ë¬¸ë§¥', 'íŒŒì•…', 'ì¤‘ìš”', 'á†¸ë‹ˆë‹¤']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ìì—°ì–´ ì²˜ë¦¬' ë²¡í„° (ì²˜ìŒ 10ì°¨ì›): [-0.008  0.009 -0.    -0.002  0.005 -0.004  0.003  0.007  0.006 -0.008]\n",
      "'ì–¸ì–´' ë²¡í„° (ì²˜ìŒ 10ì°¨ì›): [ 0.001 -0.01   0.005 -0.     0.006  0.001 -0.003  0.008  0.001 -0.   ]\n",
      "'ë°ì´í„°' ë²¡í„° (ì²˜ìŒ 10ì°¨ì›): [-0.002 -0.005  0.01  -0.009  0.005  0.005 -0.001  0.01   0.01  -0.006]\n",
      "'ì»´í“¨í„°' ë²¡í„° (ì²˜ìŒ 10ì°¨ì›): [ 0.01  -0.01  -0.006  0.003  0.007 -0.006  0.003  0.01  -0.007 -0.006]\n"
     ]
    }
   ],
   "source": [
    "# ë‹¨ì–´ë³„ ì„ë² ë”© í™•ì¸\n",
    "test_words = ['ìì—°ì–´ ì²˜ë¦¬', 'ì–¸ì–´', 'ë°ì´í„°', 'ì»´í“¨í„°']\n",
    "available_words = [word for word in test_words if word in word2vec_model.wv]\n",
    "\n",
    "for word in available_words:  \n",
    "    vector = word2vec_model.wv[word]\n",
    "    print(f\"'{word}' ë²¡í„° (ì²˜ìŒ 10ì°¨ì›): {vector[:10].round(3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'ìì—°ì–´ ì²˜ë¦¬'ì™€ ìœ ì‚¬í•œ ë‹¨ì–´ë“¤:\n",
      "  ê¸°ìˆ : 0.180\n",
      "  ë¬¸ë§¥: 0.134\n",
      "  í•™ìŠµ: 0.075\n",
      "------------------------------\n",
      "\n",
      "'ì–¸ì–´'ì™€ ìœ ì‚¬í•œ ë‹¨ì–´ë“¤:\n",
      "  á†¸ë‹ˆë‹¤: 0.240\n",
      "  ë‹¨ì–´: 0.186\n",
      "  ì˜ë¯¸: 0.172\n",
      "------------------------------\n",
      "\n",
      "'ë°ì´í„°'ì™€ ìœ ì‚¬í•œ ë‹¨ì–´ë“¤:\n",
      "  ë¶„ì•¼: 0.157\n",
      "  íŒŒì•…: 0.157\n",
      "  ì²˜ë¦¬: 0.136\n",
      "------------------------------\n",
      "\n",
      "'ì»´í“¨í„°'ì™€ ìœ ì‚¬í•œ ë‹¨ì–´ë“¤:\n",
      "  ëŸ¬ë‹: 0.176\n",
      "  ì¤‘ìš”: 0.168\n",
      "  í…ìŠ¤íŠ¸: 0.153\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ìœ ì‚¬ ë‹¨ì–´ ì°¾ê¸°\n",
    "def find_similar_words(word):\n",
    "    try:\n",
    "        similar_words = word2vec_model.wv.most_similar(word, topn=3)\n",
    "        print(f\"\\n'{word}'ì™€ ìœ ì‚¬í•œ ë‹¨ì–´ë“¤:\")\n",
    "        for similar_word, similarity in similar_words:\n",
    "            print(f\"  {similar_word}: {similarity:.3f}\")\n",
    "    except:\n",
    "        print(f\"'{word}'ì˜ ìœ ì‚¬ ë‹¨ì–´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "for word in available_words:\n",
    "    find_similar_words(word)\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 **ë¬¸ì¥ ì„ë² ë”© (Sentence Embedding)**\n",
    "\n",
    "\n",
    "- ê°œë…:\n",
    "    - ë‹¨ì–´ ì„ë² ë”©ì˜ ê°œë…ì„ í™•ì¥í•˜ì—¬ ë¬¸ì¥ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ë²¡í„°ë¡œ í‘œí˜„í•˜ëŠ” ê¸°ìˆ \n",
    "    - ë¬¸ì¥ì˜ ì˜ë¯¸ì  íŠ¹ì„±ì„ ë³´ì¡´í•˜ë©´ì„œ ê³ ì •ëœ í¬ê¸°ì˜ ë²¡í„°ë¡œ ë³€í™˜\n",
    "    - ë¹„ìŠ·í•œ ì˜ë¯¸ë¥¼ ê°€ì§„ ë¬¸ì¥ë“¤ì€ ë²¡í„° ê³µê°„ì—ì„œ ì„œë¡œ ê°€ê¹Œìš´ ê±°ë¦¬ì— ìœ„ì¹˜\n",
    "\n",
    "- ì‹¤ìŠµ: \n",
    "    - **Option1**: Word2Vec ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë‹¨ì–´ë¥¼ ë²¡í„°ë¡œ ë³€í™˜ -> ë¬¸ì¥ì„ êµ¬ì„±í•˜ëŠ” ë‹¨ì–´ ë²¡í„°ë¥¼ í‰ê·  \n",
    "    - **Option2**: Sentence-BERT(SBERT) ë“± ë¬¸ì¥ ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ì„ ë²¡í„°ë¡œ ë³€í™˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 í‰ê·  ê¸°ë°˜ ë¬¸ì¥ ì„ë² ë”©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì•ˆë‚´:**\n",
    "- ìµœì´ˆ ì‹¤í–‰ ì‹œ ëª¨ë¸ì„ ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤\n",
    "- ko-sroberta: ì•½ 500MB (ë‹¤ìš´ë¡œë“œ ì‹œê°„: 1-3ë¶„)\n",
    "- bge-m3: ì•½ 2.5GB (ë‹¤ìš´ë¡œë“œ ì‹œê°„: 5-10ë¶„)\n",
    "- ë‹¤ìš´ë¡œë“œ ìœ„ì¹˜: `~/.cache/huggingface/hub/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenize_with_kiwi ì‹¤í–‰ ì‹œê°„: 0.0018ì´ˆ\n",
      "ë¬¸ì¥: 'ìì—°ì–´ ì²˜ë¦¬ë¥¼ ê³µë¶€í•©ë‹ˆë‹¤'\n",
      "ì„ë² ë”© ì°¨ì›: 100\n",
      "ì„ë² ë”© ë²¡í„° (ì²˜ìŒ 10ì°¨ì›): [-0.004  0.005  0.003  0.004 -0.002 -0.006  0.005  0.008  0.    -0.006]\n"
     ]
    }
   ],
   "source": [
    "def create_sentence_embedding_avg(sentence, word2vec_model):\n",
    "    \"\"\"Word2Vec í‰ê· ì„ ì´ìš©í•œ ë¬¸ì¥ ì„ë² ë”©\"\"\"\n",
    "    import numpy as np\n",
    "    tokens = tokenize_with_kiwi(sentence)\n",
    "    token_words = [token.form for token in tokens if token.form in word2vec_model.wv]\n",
    "    \n",
    "    if not token_words:\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "    \n",
    "    # ë‹¨ì–´ ë²¡í„°ë“¤ì˜ í‰ê·  ê³„ì‚°\n",
    "    sentence_vector = np.zeros(word2vec_model.vector_size)\n",
    "    for word in token_words:\n",
    "        sentence_vector += word2vec_model.wv[word]\n",
    "    \n",
    "    return sentence_vector / len(token_words)\n",
    "\n",
    "# ë¬¸ì¥ ì„ë² ë”© ìƒì„±\n",
    "test_sentence = \"ìì—°ì–´ ì²˜ë¦¬ë¥¼ ê³µë¶€í•©ë‹ˆë‹¤\"\n",
    "sentence_embed = create_sentence_embedding_avg(test_sentence, word2vec_model)\n",
    "\n",
    "print(f\"ë¬¸ì¥: '{test_sentence}'\")\n",
    "print(f\"ì„ë² ë”© ì°¨ì›: {len(sentence_embed)}\")\n",
    "print(f\"ì„ë² ë”© ë²¡í„° (ì²˜ìŒ 10ì°¨ì›): {sentence_embed[:10].round(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 SBERT (Sentence-BERT) \n",
    "\n",
    "- ë¬¸ì¥ ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©: ë¬¸ì¥ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ì§ì ‘ ë³€í™˜\n",
    "- SBERT : https://sbert.net/\n",
    "- ì„¤ì¹˜ pip install sentence_transformers ë˜ëŠ” uv add sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9700ff40da34416aa5f5bc531b5a3119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëª¨ë¸ ë¡œë”© ì˜¤ë¥˜: Can't load the model for 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# í•œêµ­ì–´ ìµœì í™” ëª¨ë¸ë“¤\n",
    "korean_models = {\n",
    "    'ko-sroberta': 'jhgan/ko-sroberta-multitask',  \n",
    "    'bge-m3': 'BAAI/bge-m3'\n",
    "}\n",
    "\n",
    "def load_sentence_transformer(model_key='ko-sroberta'):\n",
    "    \"\"\"SBERT ëª¨ë¸ ë¡œë”©\"\"\"\n",
    "    try:\n",
    "        model_name = korean_models[model_key]\n",
    "        model = SentenceTransformer(model_name)\n",
    "        print(f\"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"ëª¨ë¸ ë¡œë”© ì˜¤ë¥˜: {e}\")\n",
    "        return None\n",
    "\n",
    "@measure_time\n",
    "def create_sentence_embeddings(sentences, model):\n",
    "    \"\"\"ë°°ì¹˜ ë¬¸ì¥ ì„ë² ë”© ìƒì„±\"\"\"\n",
    "    try:\n",
    "        embeddings = model.encode(sentences, convert_to_tensor=False)\n",
    "        return embeddings\n",
    "    except Exception as e:\n",
    "        print(f\"ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {e}\")\n",
    "        return None\n",
    "    \n",
    "# ko-sroberta ëª¨ë¸ ë¡œë“œ\n",
    "sbert_model = load_sentence_transformer('ko-sroberta')\n",
    "sbert_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BGE-M3 ëª¨ë¸ íŠ¹ì§•:**\n",
    "- BGE-M3ëŠ” ì¶œë ¥ ë²¡í„°ê°€ ìë™ìœ¼ë¡œ L2 ì •ê·œí™”ë©ë‹ˆë‹¤ (SentenceTransformerì˜ Normalize ë ˆì´ì–´)\n",
    "- L2 ì •ê·œí™”: ë²¡í„°ì˜ í¬ê¸°(ë…¸ë¦„)ë¥¼ 1ë¡œ ì„¤ì • â†’ ë‹¨ìœ„ ë²¡í„°ë¡œ ë³€í™˜\n",
    "- ì •ê·œí™”ëœ ë²¡í„°ì—ì„œëŠ” **ì½”ì‚¬ì¸ ìœ ì‚¬ë„ = ë‚´ì ** ê´€ê³„ê°€ ì„±ë¦½í•©ë‹ˆë‹¤\n",
    "- ë”°ë¼ì„œ BGE-M3ì˜ ìœ ì‚¬ë„ ê³„ì‚° ì‹œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ì™€ ë‚´ì ì´ ë™ì¼í•œ ê°’ì„ ê°–ìŠµë‹ˆë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: 'NoneType' object has no attribute 'encode'\n",
      "create_sentence_embeddings ì‹¤í–‰ ì‹œê°„: 0.0017ì´ˆ\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# ì„ë² ë”© ìƒì„±\u001b[39;00m\n\u001b[32m     10\u001b[39m sbert_embeddings = create_sentence_embeddings(test_sentences, sbert_model)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mì„ë² ë”© í˜•íƒœ: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43msbert_embeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mì„ë² ë”© ì°¨ì›: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msbert_embeddings.shape[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# ì²« ë²ˆì§¸ ë¬¸ì¥ì˜ ì„ë² ë”© ì¼ë¶€ ì¶œë ¥\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ë¬¸ì¥ë“¤\n",
    "test_sentences = [\n",
    "    \"ìì—°ì–´ ì²˜ë¦¬ëŠ” ì¸ê³µì§€ëŠ¥ì˜ í•µì‹¬ ê¸°ìˆ ì…ë‹ˆë‹¤\",\n",
    "    \"ë¨¸ì‹ ëŸ¬ë‹ì„ ì´ìš©í•´ì„œ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤\",\n",
    "    \"ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì •ë§ ì¢‹ë„¤ìš”\",\n",
    "    \"ì»´í“¨í„°ê°€ ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì´í•´í•˜ëŠ” ë°©ë²•ì„ ì—°êµ¬í•©ë‹ˆë‹¤\"\n",
    "]\n",
    "\n",
    "# ì„ë² ë”© ìƒì„±\n",
    "sbert_embeddings = create_sentence_embeddings(test_sentences, sbert_model)\n",
    "\n",
    "print(f\"ì„ë² ë”© í˜•íƒœ: {sbert_embeddings.shape}\")\n",
    "print(f\"ì„ë² ë”© ì°¨ì›: {sbert_embeddings.shape[1]}\")\n",
    "\n",
    "# ì²« ë²ˆì§¸ ë¬¸ì¥ì˜ ì„ë² ë”© ì¼ë¶€ ì¶œë ¥\n",
    "print(f\"\\nì²« ë²ˆì§¸ ë¬¸ì¥ ì„ë² ë”© (ì²˜ìŒ 10ì°¨ì›):\")\n",
    "print(f\"'{test_sentences[0]}'\")\n",
    "print(f\"{sbert_embeddings[0][:10].round(3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëª¨ë¸ ë¡œë”© ì˜¤ë¥˜: 'bge-m3'\n"
     ]
    }
   ],
   "source": [
    "# BGE-M3 ëª¨ë¸ ë¡œë“œ\n",
    "bge_model = load_sentence_transformer('bge-m3')\n",
    "bge_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: 'NoneType' object has no attribute 'encode'\n",
      "create_sentence_embeddings ì‹¤í–‰ ì‹œê°„: 0.0006ì´ˆ\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ì„ë² ë”© ìƒì„±\u001b[39;00m\n\u001b[32m      2\u001b[39m bge_embeddings = create_sentence_embeddings(test_sentences, bge_model)  \n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mì„ë² ë”© í˜•íƒœ: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mbge_embeddings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mì„ë² ë”© ì°¨ì›: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbge_embeddings.shape[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# ì²« ë²ˆì§¸ ë¬¸ì¥ì˜ ì„ë² ë”© ì¼ë¶€ ì¶œë ¥\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# ì„ë² ë”© ìƒì„±\n",
    "bge_embeddings = create_sentence_embeddings(test_sentences, bge_model)  \n",
    "\n",
    "print(f\"ì„ë² ë”© í˜•íƒœ: {bge_embeddings.shape}\")\n",
    "print(f\"ì„ë² ë”© ì°¨ì›: {bge_embeddings.shape[1]}\")\n",
    "\n",
    "# ì²« ë²ˆì§¸ ë¬¸ì¥ì˜ ì„ë² ë”© ì¼ë¶€ ì¶œë ¥\n",
    "print(f\"\\nì²« ë²ˆì§¸ ë¬¸ì¥ ì„ë² ë”© (ì²˜ìŒ 10ì°¨ì›):\")\n",
    "print(f\"'{test_sentences[0]}'\")\n",
    "print(f\"{bge_embeddings[0][:10].round(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ë¹„êµ\n",
    "\n",
    "### ìœ ì‚¬ë„ ë©”íŠ¸ë¦­ ë¹„êµí‘œ\n",
    "\n",
    "| ë©”íŠ¸ë¦­ | ìˆ˜ì‹ | ë²”ìœ„ | í•´ì„ | íŠ¹ì§• | ì£¼ìš” ìš©ë„ |\n",
    "|--------|------|------|------|------|----------|\n",
    "| ìœ í´ë¦¬ë“œ ê±°ë¦¬ | $\\sqrt{\\sum(a_i-b_i)^2}$ | [0, âˆ) | ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬ | ì ˆëŒ€ ê±°ë¦¬ | í´ëŸ¬ìŠ¤í„°ë§ |\n",
    "| ì½”ì‚¬ì¸ ìœ ì‚¬ë„ | $\\frac{a \\cdot b}{\\\\|a\\\\|\\\\|b\\\\|}$ | [-1, 1] | 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ ì‚¬ | ë°©í–¥ì„± ì¤‘ì‹œ | ë¬¸ì„œ ê²€ìƒ‰ |\n",
    "| ë‚´ì  | $\\sum a_i \\cdot b_i$ | (-âˆ, âˆ) | í´ìˆ˜ë¡ ìœ ì‚¬ | í¬ê¸°+ë°©í–¥ | ì¶”ì²œ ì‹œìŠ¤í…œ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 ìœ ì‚¬ë„ ë¹„êµ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í•œê¸€ í°íŠ¸ ì„¤ì • ì™„ë£Œ: AppleGothic (Darwin)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import platform\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ìë™ ê°ì§€ ë° ì„¤ì •\n",
    "def setup_korean_font():\n",
    "    \"\"\"ìš´ì˜ì²´ì œì— ë§ëŠ” í•œê¸€ í°íŠ¸ ìë™ ì„¤ì •\"\"\"\n",
    "    system = platform.system()\n",
    "    \n",
    "    if system == 'Darwin':  # macOS\n",
    "        font_name = 'AppleGothic'\n",
    "    elif system == 'Windows':\n",
    "        # Windows í°íŠ¸ ê²½ë¡œì—ì„œ malgun.ttf ì°¾ê¸°\n",
    "        font_path = 'c:/Windows/Fonts/malgun.ttf'\n",
    "        try:\n",
    "            font_name = fm.FontProperties(fname=font_path).get_name()\n",
    "        except:\n",
    "            font_name = 'Malgun Gothic'\n",
    "    else:  # Linux\n",
    "        # Linuxì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ í•œê¸€ í°íŠ¸\n",
    "        font_candidates = ['NanumGothic', 'NanumBarunGothic', 'DejaVu Sans']\n",
    "        font_name = font_candidates[0]\n",
    "    \n",
    "    plt.rc('font', family=font_name)\n",
    "    plt.rc('axes', unicode_minus=False)  # ë§ˆì´ë„ˆìŠ¤ ë¶€í˜¸ ê¹¨ì§ ë°©ì§€\n",
    "    \n",
    "    print(f\"í•œê¸€ í°íŠ¸ ì„¤ì • ì™„ë£Œ: {font_name} ({system})\")\n",
    "\n",
    "# í°íŠ¸ ì„¤ì • ì‹¤í–‰\n",
    "setup_korean_font()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import util\n",
    "\n",
    "class SimilarityCalculator:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def compare_sentence_pairs(self, sentence_pairs):\n",
    "        results = []\n",
    "        for s1, s2 in sentence_pairs:\n",
    "            # 1. ë¬¸ì¥ ì„ë² ë”© ìƒì„±\n",
    "            embeddings = self.model.encode([s1, s2], convert_to_tensor=True)\n",
    "            emb1, emb2 = embeddings[0], embeddings[1]\n",
    "\n",
    "            # 2. ë©”íŠ¸ë¦­ ê³„ì‚°\n",
    "            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„\n",
    "            cosine_score = util.cos_sim(emb1, emb2).item()\n",
    "            \n",
    "            # ë‚´ì  (Dot Product)\n",
    "            dot_score = util.dot_score(emb1, emb2).item()\n",
    "            \n",
    "            # ìœ í´ë¦¬ë“œ ê±°ë¦¬ (ê±°ë¦¬ê°€ ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ ì‚¬í•˜ë¯€ë¡œ ìˆ«ìê°€ ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ)\n",
    "            euclidean_dist = torch.dist(emb1, emb2).item()\n",
    "\n",
    "            results.append({\n",
    "                'sentence1': s1,\n",
    "                'sentence2': s2,\n",
    "                'euclidean': euclidean_dist,\n",
    "                'cosine': cosine_score,\n",
    "                'dot_product': dot_score\n",
    "            })\n",
    "        return results\n",
    "\n",
    "# sbert ëª¨ë¸ ìœ ì‚¬ë„ ê³„ì‚°ê¸° ìƒì„±\n",
    "sbert_similarity_calc = SimilarityCalculator(sbert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      2\u001b[39m sentence_pairs = [\n\u001b[32m      3\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mí•™ìƒì´ í•™êµì—ì„œ ê³µë¶€í•œë‹¤\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mí•™ìƒì´ ë„ì„œê´€ì—ì„œ ê³µë¶€í•œë‹¤\u001b[39m\u001b[33m\"\u001b[39m),    \u001b[38;5;66;03m# ìœ ì‚¬\u001b[39;00m\n\u001b[32m      4\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mìì—°ì–´ ì²˜ë¦¬ë¥¼ ë°°ìš´ë‹¤\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mì»´í“¨í„°ë¡œ ì–¸ì–´ë¥¼ ë¶„ì„í•œë‹¤\u001b[39m\u001b[33m\"\u001b[39m),        \u001b[38;5;66;03m# ìœ ì‚¬  \u001b[39;00m\n\u001b[32m      5\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë‹¤\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33më‚´ì¼ ë¹„ê°€ ì˜¬ ì˜ˆì •ì´ë‹¤\u001b[39m\u001b[33m\"\u001b[39m),             \u001b[38;5;66;03m# ê´€ë ¨\u001b[39;00m\n\u001b[32m      6\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mì˜í™”ê°€ ì¬ë¯¸ìˆë‹¤\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mìˆ˜í•™ ë¬¸ì œë¥¼ í‘¼ë‹¤\u001b[39m\u001b[33m\"\u001b[39m)                   \u001b[38;5;66;03m# ë¬´ê´€\u001b[39;00m\n\u001b[32m      7\u001b[39m ]\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# ìœ ì‚¬ë„ ë¹„êµ ì‹¤í–‰\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m sbert_comparison_results = \u001b[43msbert_similarity_calc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompare_sentence_pairs\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence_pairs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# ê²°ê³¼ ì¶œë ¥\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sbert_comparison_results, \u001b[32m1\u001b[39m):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mSimilarityCalculator.compare_sentence_pairs\u001b[39m\u001b[34m(self, sentence_pairs)\u001b[39m\n\u001b[32m     10\u001b[39m results = []\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m s1, s2 \u001b[38;5;129;01min\u001b[39;00m sentence_pairs:\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# 1. ë¬¸ì¥ ì„ë² ë”© ìƒì„±\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m([s1, s2], convert_to_tensor=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     14\u001b[39m     emb1, emb2 = embeddings[\u001b[32m0\u001b[39m], embeddings[\u001b[32m1\u001b[39m]\n\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m# 2. ë©”íŠ¸ë¦­ ê³„ì‚°\u001b[39;00m\n\u001b[32m     17\u001b[39m     \u001b[38;5;66;03m# ì½”ì‚¬ì¸ ìœ ì‚¬ë„\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ë¬¸ì¥ ìŒë“¤\n",
    "sentence_pairs = [\n",
    "    (\"í•™ìƒì´ í•™êµì—ì„œ ê³µë¶€í•œë‹¤\", \"í•™ìƒì´ ë„ì„œê´€ì—ì„œ ê³µë¶€í•œë‹¤\"),    # ìœ ì‚¬\n",
    "    (\"ìì—°ì–´ ì²˜ë¦¬ë¥¼ ë°°ìš´ë‹¤\", \"ì»´í“¨í„°ë¡œ ì–¸ì–´ë¥¼ ë¶„ì„í•œë‹¤\"),        # ìœ ì‚¬  \n",
    "    (\"ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë‹¤\", \"ë‚´ì¼ ë¹„ê°€ ì˜¬ ì˜ˆì •ì´ë‹¤\"),             # ê´€ë ¨\n",
    "    (\"ì˜í™”ê°€ ì¬ë¯¸ìˆë‹¤\", \"ìˆ˜í•™ ë¬¸ì œë¥¼ í‘¼ë‹¤\")                   # ë¬´ê´€\n",
    "]\n",
    "\n",
    "# ìœ ì‚¬ë„ ë¹„êµ ì‹¤í–‰\n",
    "sbert_comparison_results = sbert_similarity_calc.compare_sentence_pairs(sentence_pairs)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "for i, result in enumerate(sbert_comparison_results, 1):\n",
    "    print(f\"\\n{i}. ë¬¸ì¥ ë¹„êµ:\")\n",
    "    print(f\"   A: {result['sentence1']}\")\n",
    "    print(f\"   B: {result['sentence2']}\")\n",
    "    print(f\"   ìœ í´ë¦¬ë“œ ê±°ë¦¬: {result['euclidean']:.4f}\")\n",
    "    print(f\"   ì½”ì‚¬ì¸ ìœ ì‚¬ë„: {result['cosine']:.4f}\")\n",
    "    print(f\"   ë‚´ì : {result['dot_product']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bge-m3 ëª¨ë¸ ìœ ì‚¬ë„ ê³„ì‚°ê¸° ìƒì„±\n",
    "bge_similarity_calc = SimilarityCalculator(bge_model)\n",
    "\n",
    "# ìœ ì‚¬ë„ ë¹„êµ ì‹¤í–‰\n",
    "bge_comparison_results = bge_similarity_calc.compare_sentence_pairs(sentence_pairs)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "for i, result in enumerate(bge_comparison_results, 1):\n",
    "    print(f\"\\n{i}. ë¬¸ì¥ ë¹„êµ:\")\n",
    "    print(f\"   A: {result['sentence1']}\")\n",
    "    print(f\"   B: {result['sentence2']}\")\n",
    "    print(f\"   ìœ í´ë¦¬ë“œ ê±°ë¦¬: {result['euclidean']:.4f}\")\n",
    "    print(f\"   ì½”ì‚¬ì¸ ìœ ì‚¬ë„: {result['cosine']:.4f}\")\n",
    "    print(f\"   ë‚´ì : {result['dot_product']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 ìœ ì‚¬ë„ ì‹œê°í™”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# # í•œê¸€ í°íŠ¸ ì¸ì‹ - Windows\n",
    "# import matplotlib \n",
    "# font_name = matplotlib.font_manager.FontProperties(fname=\"c:/Windows/Fonts/malgun.ttf\").get_name()\n",
    "# matplotlib.rc('font', family=font_name)\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì¸ì‹ - Mac\n",
    "import matplotlib\n",
    "matplotlib.rc('font', family='AppleGothic')\n",
    "\n",
    "# ë§ˆì´ë„ˆìŠ¤ ë¶€í˜¸ ì¸ì‹\n",
    "matplotlib.rc(\"axes\", unicode_minus = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_similarity_comparison(results):\n",
    "    \"\"\"ìœ ì‚¬ë„ ë¹„êµ ì‹œê°í™”\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # ë°ì´í„° ì¤€ë¹„\n",
    "    labels = [f\"ìŒ{i+1}\" for i in range(len(results))]\n",
    "    euclidean_scores = [r['euclidean'] for r in results]\n",
    "    cosine_scores = [r['cosine'] for r in results]\n",
    "    dot_scores = [r['dot_product'] for r in results]\n",
    "    \n",
    "    # ìœ í´ë¦¬ë“œ ê±°ë¦¬ (ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬)\n",
    "    axes[0].bar(labels, euclidean_scores, color='skyblue', alpha=0.7)\n",
    "    axes[0].set_title('ìœ í´ë¦¬ë“œ ê±°ë¦¬ (ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬)')\n",
    "    axes[0].set_ylabel('ê±°ë¦¬')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (ë†’ì„ìˆ˜ë¡ ìœ ì‚¬)\n",
    "    axes[1].bar(labels, cosine_scores, color='lightgreen', alpha=0.7)\n",
    "    axes[1].set_title('ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (ë†’ì„ìˆ˜ë¡ ìœ ì‚¬)')\n",
    "    axes[1].set_ylabel('ìœ ì‚¬ë„')\n",
    "    axes[1].set_ylim(0, 1)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # ë‚´ì  (ë†’ì„ìˆ˜ë¡ ìœ ì‚¬)\n",
    "    axes[2].bar(labels, dot_scores, color='salmon', alpha=0.7)\n",
    "    axes[2].set_title('ë‚´ì  (ë†’ì„ìˆ˜ë¡ ìœ ì‚¬)')\n",
    "    axes[2].set_ylabel('ë‚´ì ê°’')\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sbert ëª¨ë¸ ê²°ê³¼ ì‹œê°í™”\n",
    "plot_similarity_comparison(sbert_comparison_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bge-m3 ëª¨ë¸ ê²°ê³¼ ì‹œê°í™”\n",
    "plot_similarity_comparison(bge_comparison_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **[ì‹¤ìŠµ í”„ë¡œì íŠ¸]**\n",
    "\n",
    "### **ë¬¸ì œ: ë¬¸ì„œ ìœ ì‚¬ë„ ë¹„êµ ì‹œìŠ¤í…œ êµ¬í˜„**\n",
    "\n",
    "**ëª©í‘œ**: ì£¼ì–´ì§„ ë¬¸ì„œë“¤ì„ í† í°í™”í•˜ê³  ì„ë² ë”©í•œ í›„ ìœ ì‚¬ë„ë¥¼ ë¹„êµí•˜ëŠ” ì‹œìŠ¤í…œ êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë¬¸ì„œ\n",
    "documents = [\n",
    "    \"ì¸ê³µì§€ëŠ¥ì€ ì»´í“¨í„° ê³¼í•™ì˜ ì¤‘ìš”í•œ ë¶„ì•¼ì…ë‹ˆë‹¤.\",\n",
    "    \"ë¨¸ì‹ ëŸ¬ë‹ì€ ì¸ê³µì§€ëŠ¥ì˜ í•˜ìœ„ ë¶„ì•¼ì…ë‹ˆë‹¤.\",\n",
    "    \"ë”¥ëŸ¬ë‹ì€ ë¨¸ì‹ ëŸ¬ë‹ì˜ í•œ ì¢…ë¥˜ì…ë‹ˆë‹¤.\",\n",
    "    \"ìì—°ì–´ ì²˜ë¦¬ëŠ” í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤.\"\n",
    "]\n",
    "\n",
    "\n",
    "# ë¬¸ì œ 1: ë¬¸ì„œë¥¼ í† í°í™”í•˜ê³  BoW ë²¡í„°ë¡œ ë³€í™˜í•˜ì‹œì˜¤\n",
    "def tokenize_documents(docs):\n",
    "    \"\"\"\n",
    "    ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ê³  BoW ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    íŒíŠ¸:\n",
    "    - Kiwi í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•˜ì—¬ ê° ë¬¸ì„œë¥¼ í† í°í™”í•˜ì„¸ìš”\n",
    "    - êµ¬ë‘ì (SF, SP íƒœê·¸)ì€ ì œì™¸í•˜ì„¸ìš”\n",
    "    - CountVectorizerë¥¼ ì‚¬ìš©í•˜ì—¬ BoW í–‰ë ¬ì„ ìƒì„±í•˜ì„¸ìš”\n",
    "    \n",
    "    Args:\n",
    "        docs: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "        \n",
    "    Returns:\n",
    "        bow_matrix: BoW ë²¡í„° í–‰ë ¬\n",
    "        vectorizer: í•™ìŠµëœ CountVectorizer ê°ì²´\n",
    "    \"\"\"\n",
    "    # ì´ ë¶€ë¶„ì„ êµ¬í˜„í•˜ì„¸ìš”\n",
    "    pass\n",
    "\n",
    "# ë¬¸ì œ 2: ë¬¸ì„œë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•˜ì‹œì˜¤\n",
    "def create_embeddings(docs):\n",
    "    \"\"\"\n",
    "    ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    íŒíŠ¸:\n",
    "    - SentenceTransformer ëª¨ë¸ì„ ë¡œë“œí•˜ì„¸ìš” (ko-sroberta ë˜ëŠ” bge-m3)\n",
    "    - model.encode() ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”\n",
    "    - ë°˜í™˜ê°’ì€ numpy ë°°ì—´ì…ë‹ˆë‹¤\n",
    "    \n",
    "    Args:\n",
    "        docs: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "        \n",
    "    Returns:\n",
    "        embeddings: ì„ë² ë”© ë²¡í„° ë°°ì—´ (n_docs, embedding_dim)\n",
    "    \"\"\"\n",
    "    # ì´ ë¶€ë¶„ì„ êµ¬í˜„í•˜ì„¸ìš”\n",
    "    pass\n",
    "\n",
    "# ë¬¸ì œ 3: ë¬¸ì„œë“¤ ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì‹œì˜¤\n",
    "def calculate_similarity(embeddings):\n",
    "    \"\"\"\n",
    "    ì„ë² ë”© ë²¡í„°ë“¤ ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    íŒíŠ¸:\n",
    "    - sklearn.metrics.pairwise.cosine_similarityë¥¼ ì‚¬ìš©í•˜ì„¸ìš”\n",
    "    - ê²°ê³¼ëŠ” (n_docs, n_docs) í¬ê¸°ì˜ ìœ ì‚¬ë„ í–‰ë ¬ì…ë‹ˆë‹¤\n",
    "    - ëŒ€ê°ì„  ê°’ì€ ìê¸° ìì‹ ê³¼ì˜ ìœ ì‚¬ë„(1.0)ì…ë‹ˆë‹¤\n",
    "    \n",
    "    Args:\n",
    "        embeddings: ì„ë² ë”© ë²¡í„° ë°°ì—´\n",
    "        \n",
    "    Returns:\n",
    "        similarity_matrix: ìœ ì‚¬ë„ í–‰ë ¬\n",
    "    \"\"\"\n",
    "    # ì´ ë¶€ë¶„ì„ êµ¬í˜„í•˜ì„¸ìš”\n",
    "    pass\n",
    "\n",
    "# ë¬¸ì œ 4: ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ìŒì„ ì°¾ìœ¼ì‹œì˜¤\n",
    "def find_most_similar_pair(similarities, docs):\n",
    "    \"\"\"\n",
    "    ìœ ì‚¬ë„ í–‰ë ¬ì—ì„œ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ìŒì„ ì°¾ìŠµë‹ˆë‹¤.\n",
    "    \n",
    "    íŒíŠ¸:\n",
    "    - ëŒ€ê°ì„  ê°’(ìê¸° ìì‹ ê³¼ì˜ ìœ ì‚¬ë„)ì€ ì œì™¸í•˜ì„¸ìš”\n",
    "    - np.fill_diagonal()ì„ ì‚¬ìš©í•˜ì—¬ ëŒ€ê°ì„ ì„ -1ë¡œ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
    "    - np.unravel_index(np.argmax(similarities), similarities.shape)ë¡œ ìµœëŒ€ê°’ì˜ ì¸ë±ìŠ¤ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
    "    \n",
    "    Args:\n",
    "        similarities: ìœ ì‚¬ë„ í–‰ë ¬\n",
    "        docs: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "        \n",
    "    Returns:\n",
    "        result: {'most_similar_pair': (ë¬¸ì„œ1, ë¬¸ì„œ2), \n",
    "                'similarity_score': ìœ ì‚¬ë„ì ìˆ˜,\n",
    "                'similarity_matrix': ìœ ì‚¬ë„í–‰ë ¬}\n",
    "    \"\"\"\n",
    "    # ì´ ë¶€ë¶„ì„ êµ¬í˜„í•˜ì„¸ìš”\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNZEEAyNjgZJ4QQC2TsgHFB",
   "mount_file_id": "1H4ZNal5I8Do5dJ2hma7tKNChye0lsRrE",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
