{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3e2d5bb",
   "metadata": {},
   "source": [
    "#  LangChain의 RAG 콤포넌트 - 텍스트 분할기 (Text Splitter) \n",
    "\n",
    "## 학습 목표\n",
    "1. 텍스트 분할의 필요성과 RAG 시스템에서의 역할 이해\n",
    "2. 다양한 텍스트 분할 전략(문자 기반, 재귀, 토큰 기반, 의미 기반) 비교\n",
    "3. 각 분할기의 파라미터(chunk_size, chunk_overlap, separator 등) 이해 및 활용\n",
    "4. 실무에서 상황에 맞는 적절한 분할 전략 선택 능력 배양\n",
    "\n",
    "### 실습 자료\n",
    "\n",
    "- data/transformer.pdf\n",
    "- data/ai.txt\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ff7b2f",
   "metadata": {},
   "source": [
    "# 환경 설정 및 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a225effd",
   "metadata": {},
   "source": [
    "`(1) Env 환경변수`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acc660f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d083b6e",
   "metadata": {},
   "source": [
    "`(2) 기본 라이브러리`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091ee72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "from pprint import pprint\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f742b3e",
   "metadata": {},
   "source": [
    "`(3) 문서 로드`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8ff006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# PDF 로더 초기화\n",
    "pdf_loader = PyPDFLoader('./data/transformer.pdf', mode=\"single\")\n",
    "\n",
    "# 동기 로딩\n",
    "pdf_docs = pdf_loader.load()\n",
    "print(f'PDF 문서 개수: {len(pdf_docs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aae49f4",
   "metadata": {},
   "source": [
    "# 텍스트 분할(Text Splitting) \n",
    "\n",
    "- 대규모 텍스트 문서를 처리할 때 매우 중요한 전처리 단계\n",
    "- 고려사항:\n",
    "    1. 문서의 구조와 형식\n",
    "    2. 원하는 청크 크기\n",
    "    3. 문맥 보존의 중요도\n",
    "    4. 처리 속도 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7e8db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'첫 번째 문서: {pdf_docs[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f75d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa4c953",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_text = pdf_docs[0].page_content\n",
    "print(f'첫 번째 문서의 텍스트 길이: {len(long_text)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7e01d4",
   "metadata": {},
   "source": [
    "### 1. **CharacterTextSplitter**\n",
    "- 가장 기본적인 분할 방식\n",
    "- 문자 수를 기준으로 텍스트를 분할\n",
    "- 단순하지만 문맥을 고려하지 않는다는 단점이 있음\n",
    "\n",
    "- 설치: pip install langchain_text_splitters 또는 uv add langchain_text_splitters\n",
    "\n",
    "- CharacterTextSplitter 주요 파라미터\n",
    "\n",
    "    | 파라미터 | 기본값 | 설명 |\n",
    "    |---------|--------|------|\n",
    "    | `separator` | `\"\\n\\n\"` | 청크를 구분하는 구분자 |\n",
    "    | `chunk_size` | `1000` | 각 청크의 최대 길이 (문자 수) |\n",
    "    | `chunk_overlap` | `200` | 인접 청크 간 중복되는 문자 수 (문맥 보존) |\n",
    "    | `length_function` | `len` | 텍스트 길이를 계산하는 함수 |\n",
    "    | `is_separator_regex` | `False` | separator가 정규표현식인지 여부 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47ca7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter \n",
    "\n",
    "# 텍스트 분할기 초기화 (기본 설정값 적용 )\n",
    "text_splitter = CharacterTextSplitter(\n",
    "\n",
    "    # CharacterTextSplitter의 기본 설정값\n",
    "    separator = \"\\n\\n\",         # 청크 구분자: 두 개의 개행문자\n",
    "    is_separator_regex = False,  # 구분자가 정규식인지 여부\n",
    "\n",
    "    # TextSplitter의 기본 설정값\n",
    "    chunk_size = 1000,          # 청크 길이\n",
    "    chunk_overlap = 200,        # 청크 중첩\n",
    "    length_function = len,      # 길이 함수 (문자열 길이)\n",
    "    keep_separator = False,     # 구분자 유지 여부\n",
    "    add_start_index = False,    # 시작 인덱스 추가 여부\n",
    "    strip_whitespace = True,    # 공백 제거 여부\n",
    ")\n",
    "\n",
    "# 텍스트 분할 - split_text() 메서드 사용\n",
    "texts = text_splitter.split_text(long_text)\n",
    "\n",
    "# 분할된 텍스트 개수 출력\n",
    "print(f'분할된 텍스트 개수: {len(texts)}')\n",
    "\n",
    "# 첫 번째 분할된 텍스트 출력\n",
    "print(f'첫 번째 분할된 텍스트: {texts[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e962d5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dcoument 객체 출력 (첫 번째 분할된 텍스트로 생성)\n",
    "pdf_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e77ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# 문장 구분자를 개행문자로 설정\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = \" \",        # 청크 구분자: 개행문자\n",
    "    chunk_size = 1000,       # 청크 길이\n",
    "    chunk_overlap = 200      # 청크 중첩\n",
    ")\n",
    "\n",
    "# split_documents() 메서드 사용 : Document 객체를 여러 개의 작은 청크 문서로 분할\n",
    "chunks = text_splitter.split_documents([pdf_docs[0]])   # 첫 번째 문서만 분할\n",
    "\n",
    "# 분할된 텍스트 개수 출력\n",
    "print(f'분할된 텍스트 개수: {len(chunks)}')\n",
    "\n",
    "# 각 청크의 텍스트 길이 출력\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f'청크 {i+1}의 텍스트 길이: {len(chunk.page_content)}')\n",
    "\n",
    "# 첫 번째 청크의 텍스트 출력\n",
    "print(f'첫 번째 청크의 텍스트: {chunks[0].page_content}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b26edb",
   "metadata": {},
   "source": [
    "### 2. **RecursiveCharacterTextSplitter**\n",
    "\n",
    "- 재귀적으로 텍스트를 분할\n",
    "- 구분자를 순차적으로 적용하여 큰 청크에서 시작하여 점진적으로 더 작은 단위로 분할\n",
    "- 문맥을 더 잘 보존할 수 있음  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52da03ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 재귀적 텍스트 분할기 초기화\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,             # 청크 크기  \n",
    "    chunk_overlap=200,           # 청크 중 중복되는 부분 크기\n",
    "    length_function=len,         # 글자 수를 기준으로 분할\n",
    "    separators=[\"\\n\\n\",  \"\\n\", \" \", \"\"],  # 구분자 - 재귀적으로 순차적으로 적용 \n",
    ")\n",
    "\n",
    "# split_documents() 메서드 사용 : Document 객체를 여러 개의 작은 청크 문서로 분할\n",
    "chunks = text_splitter.split_documents(pdf_docs)\n",
    "print(f\"생성된 텍스트 청크 수: {len(chunks)}\")\n",
    "print(f\"각 청크의 길이: {list(len(chunk.page_content) for chunk in chunks)}\")\n",
    "print()\n",
    "\n",
    "# 각 청크의 시작 부분과 끝 부분 확인 - 5개 청크만 출력\n",
    "for chunk in chunks[:5]:\n",
    "    print(chunk.page_content[:200])\n",
    "    print(\"-\" * 100)\n",
    "    print(chunk.page_content[-200:])\n",
    "    print(\"=\" * 100)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d5dc7c",
   "metadata": {},
   "source": [
    "### 3. **정규표현식 사용**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d1fc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# 문장을 구분하여 분할 - 정규표현식 사용 (문장 구분자: 마침표, 느낌표, 물음표 다음에 공백이 오는 경우)\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=r'(?<=[.!?])\\s+',  # 각 Document 객체의 page_content 속성을 문장으로 분할\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    is_separator_regex=True,      # 구분자가 정규식인지 여부: True\n",
    "    keep_separator=True,          # 구분자 유지 여부: True\n",
    ")\n",
    "\n",
    "# split_documents() 메서드 사용 : Document 객체를 여러 개의 작은 청크 문서로 분할\n",
    "chunks = text_splitter.split_documents(pdf_docs)  # 모든 문서를 분할\n",
    "print(f\"생성된 텍스트 청크 수: {len(chunks)}\")\n",
    "print(f\"각 청크의 길이: {list(len(chunk.page_content) for chunk in chunks)}\")\n",
    "print()\n",
    "\n",
    "# 각 청크의 시작 부분과 끝 부분 확인 - 5개 청크만 출력\n",
    "for chunk in chunks[:5]:\n",
    "    print(chunk.page_content[:200])\n",
    "    print(\"-\" * 100)\n",
    "    print(chunk.page_content[-200:])\n",
    "    print(\"=\" * 100)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0cb712",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbe562f",
   "metadata": {},
   "source": [
    "### 4. **토큰 수를 기반으로 분할**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9198b73",
   "metadata": {},
   "source": [
    "`(1) tiktoken`  \n",
    "- OpenAI에서 만든 BPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b2d060",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pdf_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdea518c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫번째 문서 객체의 텍스트 길이\n",
    "len(pdf_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b5fda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# TikToken 인코더를 사용하여 재귀적 텍스트 분할기 초기화\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name=\"cl100k_base\", \n",
    "    # model_name=\"gpt-4.1\",\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=0,\n",
    ")\n",
    "\n",
    "# split_documents() 메서드 사용 : Document 객체를 여러 개의 작은 청크 문서로 분할\n",
    "chunks = text_splitter.split_documents([pdf_docs[0]])  # 첫 번째 문서만 분할\n",
    "\n",
    "print(f\"생성된 청크 수: {len(chunks)}\")\n",
    "print(f\"각 청크의 길이: {list(len(chunk.page_content) for chunk in chunks)}\")\n",
    "\n",
    "# 각 청크의 시작 부분과 끝 부분 확인\n",
    "for chunk in chunks[:5]:\n",
    "    print(chunk.page_content[:50])\n",
    "    print(\"-\" * 50)\n",
    "    print(chunk.page_content[-50:])\n",
    "    print(\"=\" * 50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10e8b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "# tokenizer = tiktoken.encoding_for_model(\"gpt-4.1\")\n",
    "\n",
    "for chunk in chunks[:5]:\n",
    "\n",
    "    # 각 청크를 토큰화\n",
    "    tokens = tokenizer.encode(chunk.page_content)\n",
    "    # 각 청크의 단어 수 확인\n",
    "    print(len(tokens))\n",
    "    # 각 청크의 토큰화 결과 확인 (첫 10개 토큰만 출력)\n",
    "    print(tokens[:10])\n",
    "    # 토큰 ID를 실제 토큰(문자열)로 변환해서 출력\n",
    "    token_strings = [tokenizer.decode([token]) for token in tokens[:10]]\n",
    "    print(token_strings)\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbb721a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d879f133",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chunk.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b01677",
   "metadata": {},
   "source": [
    "`(2) Hugging Face 토크나이저`  \n",
    "- Hugging Face tokenizer 모델의 토큰 수를 기준으로 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6872bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-m3\")\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1477bfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 인코딩 - 문장을 토큰(ID)으로 변환\n",
    "tokens = tokenizer.encode(\"안녕하세요. 반갑습니다.\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07cafbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰을 출력 (토큰 ID를 실제 토큰(문자열)로 변환)\n",
    "print(tokenizer.convert_ids_to_tokens(tokens)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcf6a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코딩 - 토큰을 문자열로 변환\n",
    "print(tokenizer.decode(tokens, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311dec97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Huggingface 토크나이저를 사용하여 재귀적 텍스트 분할기 초기화\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer=tokenizer,\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=0,\n",
    ")\n",
    "\n",
    "# split_documents() 메서드 사용 : Document 객체를 여러 개의 작은 청크 문서로 분할\n",
    "chunks = text_splitter.split_documents([pdf_docs[0]]) # 첫 번째 문서만 분할\n",
    "\n",
    "print(f\"생성된 청크 수: {len(chunks)}\")\n",
    "print(f\"각 청크의 길이: {list(len(chunk.page_content) for chunk in chunks)}\")\n",
    "print()\n",
    "\n",
    "for chunk in chunks[:5]:\n",
    "\n",
    "    # 각 청크를 토큰화\n",
    "    tokens = tokenizer.encode(chunk.page_content)\n",
    "    # 각 청크의 단어 수 확인\n",
    "    print(len(tokens))\n",
    "    # 각 청크의 토큰화 결과 확인 (첫 10개 토큰만 출력)\n",
    "    print(tokens[:10])\n",
    "    # 토큰 ID를 실제 토큰(문자열)로 변환해서 출력\n",
    "    token_strings = tokenizer.convert_ids_to_tokens(tokens[:10]) \n",
    "    print(token_strings)\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dd86bc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576a09ca",
   "metadata": {},
   "source": [
    "# [실습 프로젝트]\n",
    "\n",
    "### 실습 목표\n",
    "다양한 텍스트 분할 전략을 직접 구현하고 비교하면서, 각 분할기의 특성과 적합한 사용 사례를 이해합니다.\n",
    "\n",
    "### 실습 단계\n",
    "1. **문서 로드**: data/ai.txt 파일을 읽어서 랭체인 Document 객체로 변환\n",
    "2. **다음 세 가지 분할기 구현**:\n",
    "   - RecursiveCharacterTextSplitter (chunk_size=500, chunk_overlap=50)\n",
    "   - CharacterTextSplitter (정규식 사용, 문장 단위 분할)\n",
    "3. **결과 비교 분석**:\n",
    "   - 각 분할기로 생성된 청크 개수\n",
    "   - 첫 3개 청크의 시작 부분 비교\n",
    "   - 각 분할기의 장단점 분석\n",
    "\n",
    "### 힌트\n",
    "- TextLoader를 사용하여 텍스트 파일 로드\n",
    "- 각 분할기의 split_documents() 메서드 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dd7358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기에 코드를 작성하세요.\n",
    "\n",
    "# 1. 문서 로드\n",
    "\n",
    "\n",
    "# 2. RecursiveCharacterTextSplitter 구현\n",
    "\n",
    "\n",
    "# 3. CharacterTextSplitter (정규식) 구현\n",
    "\n",
    "\n",
    "# 4. 결과 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff961d8e",
   "metadata": {},
   "source": [
    "### chunk_size와 chunk_overlap 설정 가이드\n",
    "- **chunk_size 권장 범위**: 500-1000 문자 또는 200-500 토큰\n",
    "- **chunk_overlap 권장**: chunk_size의 10-20%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
